{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(dim):\n",
    "    baseline_model = nn.Sequential(\n",
    "        nn.Linear(dim, 256, dtype=torch.float64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 64, dtype=torch.float64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 64, dtype=torch.float64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 256, dtype=torch.float64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 1, dtype=torch.float64),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    return baseline_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка данных 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = loadarff(\"data/electricity-normalized.arff\")\n",
    "df_data = pd.DataFrame(raw_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['day'] = df_data['day'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "      <th>period</th>\n",
       "      <th>nswprice</th>\n",
       "      <th>nswdemand</th>\n",
       "      <th>vicprice</th>\n",
       "      <th>vicdemand</th>\n",
       "      <th>transfer</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056443</td>\n",
       "      <td>0.439155</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.422915</td>\n",
       "      <td>0.414912</td>\n",
       "      <td>b'UP'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.051699</td>\n",
       "      <td>0.415055</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.422915</td>\n",
       "      <td>0.414912</td>\n",
       "      <td>b'UP'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.051489</td>\n",
       "      <td>0.385004</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.422915</td>\n",
       "      <td>0.414912</td>\n",
       "      <td>b'UP'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.045485</td>\n",
       "      <td>0.314639</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.422915</td>\n",
       "      <td>0.414912</td>\n",
       "      <td>b'UP'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042482</td>\n",
       "      <td>0.251116</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.422915</td>\n",
       "      <td>0.414912</td>\n",
       "      <td>b'DOWN'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45307</th>\n",
       "      <td>0.9158</td>\n",
       "      <td>7</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.044224</td>\n",
       "      <td>0.340672</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.255049</td>\n",
       "      <td>0.405263</td>\n",
       "      <td>b'DOWN'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45308</th>\n",
       "      <td>0.9158</td>\n",
       "      <td>7</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.044884</td>\n",
       "      <td>0.355549</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.241326</td>\n",
       "      <td>0.420614</td>\n",
       "      <td>b'DOWN'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45309</th>\n",
       "      <td>0.9158</td>\n",
       "      <td>7</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.043593</td>\n",
       "      <td>0.340970</td>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.247799</td>\n",
       "      <td>0.362281</td>\n",
       "      <td>b'DOWN'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45310</th>\n",
       "      <td>0.9158</td>\n",
       "      <td>7</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.066651</td>\n",
       "      <td>0.329366</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.345417</td>\n",
       "      <td>0.206579</td>\n",
       "      <td>b'UP'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45311</th>\n",
       "      <td>0.9158</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050679</td>\n",
       "      <td>0.288753</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.355256</td>\n",
       "      <td>0.231140</td>\n",
       "      <td>b'DOWN'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45312 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  day    period  nswprice  nswdemand  vicprice  vicdemand  \\\n",
       "0      0.0000    2  0.000000  0.056443   0.439155  0.003467   0.422915   \n",
       "1      0.0000    2  0.021277  0.051699   0.415055  0.003467   0.422915   \n",
       "2      0.0000    2  0.042553  0.051489   0.385004  0.003467   0.422915   \n",
       "3      0.0000    2  0.063830  0.045485   0.314639  0.003467   0.422915   \n",
       "4      0.0000    2  0.085106  0.042482   0.251116  0.003467   0.422915   \n",
       "...       ...  ...       ...       ...        ...       ...        ...   \n",
       "45307  0.9158    7  0.914894  0.044224   0.340672  0.003033   0.255049   \n",
       "45308  0.9158    7  0.936170  0.044884   0.355549  0.003072   0.241326   \n",
       "45309  0.9158    7  0.957447  0.043593   0.340970  0.002983   0.247799   \n",
       "45310  0.9158    7  0.978723  0.066651   0.329366  0.004630   0.345417   \n",
       "45311  0.9158    7  1.000000  0.050679   0.288753  0.003542   0.355256   \n",
       "\n",
       "       transfer    class  \n",
       "0      0.414912    b'UP'  \n",
       "1      0.414912    b'UP'  \n",
       "2      0.414912    b'UP'  \n",
       "3      0.414912    b'UP'  \n",
       "4      0.414912  b'DOWN'  \n",
       "...         ...      ...  \n",
       "45307  0.405263  b'DOWN'  \n",
       "45308  0.420614  b'DOWN'  \n",
       "45309  0.362281  b'DOWN'  \n",
       "45310  0.206579    b'UP'  \n",
       "45311  0.231140  b'DOWN'  \n",
       "\n",
       "[45312 rows x 9 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_data['class'].to_numpy()\n",
    "target[target==b'UP'] = 1\n",
    "target[target==b'DOWN'] = 0\n",
    "target = target.astype(dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = df_data[['date', 'day', 'period', 'nswprice', 'nswdemand', 'vicprice', 'vicdemand', 'transfer']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные для первого датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = loadarff(\"data/phpkIxskf.arff\")\n",
    "df_data = pd.DataFrame(raw_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[target==b'1'] = 0\n",
    "target[target==b'2'] = 1\n",
    "target = target.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_data.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58.0</td>\n",
       "      <td>b'management'</td>\n",
       "      <td>b'married'</td>\n",
       "      <td>b'tertiary'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>2143.0</td>\n",
       "      <td>b'yes'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'unknown'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>b'may'</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'unknown'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.0</td>\n",
       "      <td>b'technician'</td>\n",
       "      <td>b'single'</td>\n",
       "      <td>b'secondary'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>29.0</td>\n",
       "      <td>b'yes'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'unknown'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>b'may'</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'unknown'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.0</td>\n",
       "      <td>b'entrepreneur'</td>\n",
       "      <td>b'married'</td>\n",
       "      <td>b'secondary'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'yes'</td>\n",
       "      <td>b'yes'</td>\n",
       "      <td>b'unknown'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>b'may'</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'unknown'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.0</td>\n",
       "      <td>b'blue-collar'</td>\n",
       "      <td>b'married'</td>\n",
       "      <td>b'unknown'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>b'yes'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'unknown'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>b'may'</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'unknown'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.0</td>\n",
       "      <td>b'unknown'</td>\n",
       "      <td>b'single'</td>\n",
       "      <td>b'unknown'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'unknown'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>b'may'</td>\n",
       "      <td>198.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'unknown'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45206</th>\n",
       "      <td>51.0</td>\n",
       "      <td>b'technician'</td>\n",
       "      <td>b'married'</td>\n",
       "      <td>b'tertiary'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>825.0</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'cellular'</td>\n",
       "      <td>17.0</td>\n",
       "      <td>b'nov'</td>\n",
       "      <td>977.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'unknown'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45207</th>\n",
       "      <td>71.0</td>\n",
       "      <td>b'retired'</td>\n",
       "      <td>b'divorced'</td>\n",
       "      <td>b'primary'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>1729.0</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'cellular'</td>\n",
       "      <td>17.0</td>\n",
       "      <td>b'nov'</td>\n",
       "      <td>456.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'unknown'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45208</th>\n",
       "      <td>72.0</td>\n",
       "      <td>b'retired'</td>\n",
       "      <td>b'married'</td>\n",
       "      <td>b'secondary'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>5715.0</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'cellular'</td>\n",
       "      <td>17.0</td>\n",
       "      <td>b'nov'</td>\n",
       "      <td>1127.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>b'success'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45209</th>\n",
       "      <td>57.0</td>\n",
       "      <td>b'blue-collar'</td>\n",
       "      <td>b'married'</td>\n",
       "      <td>b'secondary'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>668.0</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'telephone'</td>\n",
       "      <td>17.0</td>\n",
       "      <td>b'nov'</td>\n",
       "      <td>508.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'unknown'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45210</th>\n",
       "      <td>37.0</td>\n",
       "      <td>b'entrepreneur'</td>\n",
       "      <td>b'married'</td>\n",
       "      <td>b'secondary'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>2971.0</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'no'</td>\n",
       "      <td>b'cellular'</td>\n",
       "      <td>17.0</td>\n",
       "      <td>b'nov'</td>\n",
       "      <td>361.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>b'other'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45211 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1               V2           V3            V4     V5      V6  \\\n",
       "0      58.0    b'management'   b'married'   b'tertiary'  b'no'  2143.0   \n",
       "1      44.0    b'technician'    b'single'  b'secondary'  b'no'    29.0   \n",
       "2      33.0  b'entrepreneur'   b'married'  b'secondary'  b'no'     2.0   \n",
       "3      47.0   b'blue-collar'   b'married'    b'unknown'  b'no'  1506.0   \n",
       "4      33.0       b'unknown'    b'single'    b'unknown'  b'no'     1.0   \n",
       "...     ...              ...          ...           ...    ...     ...   \n",
       "45206  51.0    b'technician'   b'married'   b'tertiary'  b'no'   825.0   \n",
       "45207  71.0       b'retired'  b'divorced'    b'primary'  b'no'  1729.0   \n",
       "45208  72.0       b'retired'   b'married'  b'secondary'  b'no'  5715.0   \n",
       "45209  57.0   b'blue-collar'   b'married'  b'secondary'  b'no'   668.0   \n",
       "45210  37.0  b'entrepreneur'   b'married'  b'secondary'  b'no'  2971.0   \n",
       "\n",
       "           V7      V8            V9   V10     V11     V12  V13    V14   V15  \\\n",
       "0      b'yes'   b'no'    b'unknown'   5.0  b'may'   261.0  1.0   -1.0   0.0   \n",
       "1      b'yes'   b'no'    b'unknown'   5.0  b'may'   151.0  1.0   -1.0   0.0   \n",
       "2      b'yes'  b'yes'    b'unknown'   5.0  b'may'    76.0  1.0   -1.0   0.0   \n",
       "3      b'yes'   b'no'    b'unknown'   5.0  b'may'    92.0  1.0   -1.0   0.0   \n",
       "4       b'no'   b'no'    b'unknown'   5.0  b'may'   198.0  1.0   -1.0   0.0   \n",
       "...       ...     ...           ...   ...     ...     ...  ...    ...   ...   \n",
       "45206   b'no'   b'no'   b'cellular'  17.0  b'nov'   977.0  3.0   -1.0   0.0   \n",
       "45207   b'no'   b'no'   b'cellular'  17.0  b'nov'   456.0  2.0   -1.0   0.0   \n",
       "45208   b'no'   b'no'   b'cellular'  17.0  b'nov'  1127.0  5.0  184.0   3.0   \n",
       "45209   b'no'   b'no'  b'telephone'  17.0  b'nov'   508.0  4.0   -1.0   0.0   \n",
       "45210   b'no'   b'no'   b'cellular'  17.0  b'nov'   361.0  2.0  188.0  11.0   \n",
       "\n",
       "              V16  \n",
       "0      b'unknown'  \n",
       "1      b'unknown'  \n",
       "2      b'unknown'  \n",
       "3      b'unknown'  \n",
       "4      b'unknown'  \n",
       "...           ...  \n",
       "45206  b'unknown'  \n",
       "45207  b'unknown'  \n",
       "45208  b'success'  \n",
       "45209  b'unknown'  \n",
       "45210    b'other'  \n",
       "\n",
       "[45211 rows x 16 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = features[['V1', 'V6', 'V10', 'V12', 'V13', 'V14', 'V15']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_model = baseline(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "num_epochs = 2000\n",
    "min_loss, t = np.inf, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(b_model.parameters(), lr=1e-4, eps=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=7, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=256, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (9): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([58., 44., 33., ..., 72., 57., 37.]),\n",
       " array([b'management', b'technician', b'entrepreneur', ..., b'retired',\n",
       "        b'blue-collar', b'entrepreneur'], dtype=object),\n",
       " array([b'married', b'single', b'married', ..., b'married', b'married',\n",
       "        b'married'], dtype=object),\n",
       " array([b'tertiary', b'secondary', b'secondary', ..., b'secondary',\n",
       "        b'secondary', b'secondary'], dtype=object),\n",
       " array([b'no', b'no', b'no', ..., b'no', b'no', b'no'], dtype=object),\n",
       " array([2.143e+03, 2.900e+01, 2.000e+00, ..., 5.715e+03, 6.680e+02,\n",
       "        2.971e+03]),\n",
       " array([b'yes', b'yes', b'yes', ..., b'no', b'no', b'no'], dtype=object),\n",
       " array([b'no', b'no', b'yes', ..., b'no', b'no', b'no'], dtype=object),\n",
       " array([b'unknown', b'unknown', b'unknown', ..., b'cellular', b'telephone',\n",
       "        b'cellular'], dtype=object),\n",
       " array([ 5.,  5.,  5., ..., 17., 17., 17.]),\n",
       " array([b'may', b'may', b'may', ..., b'nov', b'nov', b'nov'], dtype=object),\n",
       " array([ 261.,  151.,   76., ..., 1127.,  508.,  361.]),\n",
       " array([1., 1., 1., ..., 5., 4., 2.]),\n",
       " array([ -1.,  -1.,  -1., ..., 184.,  -1., 188.]),\n",
       " array([ 0.,  0.,  0., ...,  3.,  0., 11.]),\n",
       " array([b'unknown', b'unknown', b'unknown', ..., b'success', b'unknown',\n",
       "        b'other'], dtype=object)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[features[key].values for key in features.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4428\\982722030.py:2: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if dtype == np.object:  # Only process byte object columns.\n"
     ]
    }
   ],
   "source": [
    "for col, dtype in features.dtypes.items():\n",
    "    if dtype == np.object:  # Only process byte object columns.\n",
    "        features[col] = features[col].apply(lambda x: x.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[['V1', 'V6', 'V10', 'V12', 'V13', 'V14', 'V15']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tensors = [torch.tensor(feat[key].values) for key in feat.keys()]\n",
    "grid_tensor = torch.stack(grid_tensors)\n",
    "grid_flattened = grid_tensor.view(grid_tensor.shape[0], -1).transpose(0, 1)\n",
    "grid_flattened = grid_flattened.to(grid_flattened.to(torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.8000e+01,  2.1430e+03,  5.0000e+00,  2.6100e+02,  1.0000e+00,\n",
       "        -1.0000e+00,  0.0000e+00], dtype=torch.float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_flattened[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target)\n",
    "train_features = grid_flattened[:36168]\n",
    "train_target = target[:36168]\n",
    "test_features = grid_flattened[36168:]\n",
    "test_target = target[36168:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.to(torch.float64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Третьи данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = loadarff(\"data/phpSSK7iA.arff\")\n",
    "df_data = pd.DataFrame(raw_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>...</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.497009</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132956</td>\n",
       "      <td>0.678031</td>\n",
       "      <td>0.273166</td>\n",
       "      <td>0.585445</td>\n",
       "      <td>0.743663</td>\n",
       "      <td>0.243144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.606291</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111209</td>\n",
       "      <td>0.803455</td>\n",
       "      <td>0.106105</td>\n",
       "      <td>0.411754</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>0.106480</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.480124</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209791</td>\n",
       "      <td>0.610350</td>\n",
       "      <td>0.356453</td>\n",
       "      <td>0.517720</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>0.352308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538825</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.196344</td>\n",
       "      <td>0.724230</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.288764</td>\n",
       "      <td>0.805110</td>\n",
       "      <td>0.208989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.517794</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>0.781422</td>\n",
       "      <td>0.154361</td>\n",
       "      <td>0.303809</td>\n",
       "      <td>0.812646</td>\n",
       "      <td>0.125177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.506409</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209887</td>\n",
       "      <td>0.633426</td>\n",
       "      <td>0.297659</td>\n",
       "      <td>0.376124</td>\n",
       "      <td>0.727093</td>\n",
       "      <td>0.308163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.651023</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151154</td>\n",
       "      <td>0.766505</td>\n",
       "      <td>0.170876</td>\n",
       "      <td>0.404546</td>\n",
       "      <td>0.787935</td>\n",
       "      <td>0.192527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.520564</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.179949</td>\n",
       "      <td>0.768785</td>\n",
       "      <td>0.177341</td>\n",
       "      <td>0.471179</td>\n",
       "      <td>0.872241</td>\n",
       "      <td>0.122132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.765646</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536954</td>\n",
       "      <td>0.634936</td>\n",
       "      <td>0.342713</td>\n",
       "      <td>0.447162</td>\n",
       "      <td>0.672689</td>\n",
       "      <td>0.372936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.533952</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347966</td>\n",
       "      <td>0.757971</td>\n",
       "      <td>0.230667</td>\n",
       "      <td>0.272652</td>\n",
       "      <td>0.854116</td>\n",
       "      <td>0.140316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3751 rows × 1777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            D1        D2    D3   D4        D5        D6        D7        D8  \\\n",
       "0     0.000000  0.497009  0.10  0.0  0.132956  0.678031  0.273166  0.585445   \n",
       "1     0.366667  0.606291  0.05  0.0  0.111209  0.803455  0.106105  0.411754   \n",
       "2     0.033300  0.480124  0.00  0.0  0.209791  0.610350  0.356453  0.517720   \n",
       "3     0.000000  0.538825  0.00  0.5  0.196344  0.724230  0.235606  0.288764   \n",
       "4     0.100000  0.517794  0.00  0.0  0.494734  0.781422  0.154361  0.303809   \n",
       "...        ...       ...   ...  ...       ...       ...       ...       ...   \n",
       "3746  0.033300  0.506409  0.10  0.0  0.209887  0.633426  0.297659  0.376124   \n",
       "3747  0.133333  0.651023  0.15  0.0  0.151154  0.766505  0.170876  0.404546   \n",
       "3748  0.200000  0.520564  0.00  0.0  0.179949  0.768785  0.177341  0.471179   \n",
       "3749  0.100000  0.765646  0.00  0.0  0.536954  0.634936  0.342713  0.447162   \n",
       "3750  0.133333  0.533952  0.00  0.0  0.347966  0.757971  0.230667  0.272652   \n",
       "\n",
       "            D9       D10  ...  D1768  D1769  D1770  D1771  D1772  D1773  \\\n",
       "0     0.743663  0.243144  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1     0.836582  0.106480  ...    1.0    1.0    1.0    0.0    1.0    0.0   \n",
       "2     0.679051  0.352308  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3     0.805110  0.208989  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4     0.812646  0.125177  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...        ...       ...  ...    ...    ...    ...    ...    ...    ...   \n",
       "3746  0.727093  0.308163  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3747  0.787935  0.192527  ...    0.0    1.0    0.0    1.0    0.0    1.0   \n",
       "3748  0.872241  0.122132  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3749  0.672689  0.372936  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3750  0.854116  0.140316  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "      D1774  D1775  D1776  target  \n",
       "0       0.0    0.0    0.0    b'1'  \n",
       "1       0.0    1.0    0.0    b'1'  \n",
       "2       0.0    0.0    0.0    b'1'  \n",
       "3       0.0    0.0    0.0    b'1'  \n",
       "4       0.0    0.0    0.0    b'0'  \n",
       "...     ...    ...    ...     ...  \n",
       "3746    0.0    0.0    0.0    b'1'  \n",
       "3747    0.0    0.0    0.0    b'1'  \n",
       "3748    0.0    0.0    0.0    b'0'  \n",
       "3749    0.0    0.0    0.0    b'1'  \n",
       "3750    0.0    0.0    0.0    b'0'  \n",
       "\n",
       "[3751 rows x 1777 columns]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9228\\1814935476.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target[target==b'1'] = 1\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9228\\1814935476.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target[target==b'0'] = 0\n"
     ]
    }
   ],
   "source": [
    "target = df_data['target']\n",
    "target[target==b'1'] = 1\n",
    "target[target==b'0'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = list(df_data.keys())\n",
    "ks = ks[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = df_data[ks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>...</th>\n",
       "      <th>D1767</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.497009</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132956</td>\n",
       "      <td>0.678031</td>\n",
       "      <td>0.273166</td>\n",
       "      <td>0.585445</td>\n",
       "      <td>0.743663</td>\n",
       "      <td>0.243144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.606291</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111209</td>\n",
       "      <td>0.803455</td>\n",
       "      <td>0.106105</td>\n",
       "      <td>0.411754</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>0.106480</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.480124</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209791</td>\n",
       "      <td>0.610350</td>\n",
       "      <td>0.356453</td>\n",
       "      <td>0.517720</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>0.352308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538825</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.196344</td>\n",
       "      <td>0.724230</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.288764</td>\n",
       "      <td>0.805110</td>\n",
       "      <td>0.208989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.517794</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>0.781422</td>\n",
       "      <td>0.154361</td>\n",
       "      <td>0.303809</td>\n",
       "      <td>0.812646</td>\n",
       "      <td>0.125177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.506409</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209887</td>\n",
       "      <td>0.633426</td>\n",
       "      <td>0.297659</td>\n",
       "      <td>0.376124</td>\n",
       "      <td>0.727093</td>\n",
       "      <td>0.308163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.651023</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151154</td>\n",
       "      <td>0.766505</td>\n",
       "      <td>0.170876</td>\n",
       "      <td>0.404546</td>\n",
       "      <td>0.787935</td>\n",
       "      <td>0.192527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.520564</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.179949</td>\n",
       "      <td>0.768785</td>\n",
       "      <td>0.177341</td>\n",
       "      <td>0.471179</td>\n",
       "      <td>0.872241</td>\n",
       "      <td>0.122132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.765646</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536954</td>\n",
       "      <td>0.634936</td>\n",
       "      <td>0.342713</td>\n",
       "      <td>0.447162</td>\n",
       "      <td>0.672689</td>\n",
       "      <td>0.372936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.533952</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347966</td>\n",
       "      <td>0.757971</td>\n",
       "      <td>0.230667</td>\n",
       "      <td>0.272652</td>\n",
       "      <td>0.854116</td>\n",
       "      <td>0.140316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3751 rows × 1776 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            D1        D2    D3   D4        D5        D6        D7        D8  \\\n",
       "0     0.000000  0.497009  0.10  0.0  0.132956  0.678031  0.273166  0.585445   \n",
       "1     0.366667  0.606291  0.05  0.0  0.111209  0.803455  0.106105  0.411754   \n",
       "2     0.033300  0.480124  0.00  0.0  0.209791  0.610350  0.356453  0.517720   \n",
       "3     0.000000  0.538825  0.00  0.5  0.196344  0.724230  0.235606  0.288764   \n",
       "4     0.100000  0.517794  0.00  0.0  0.494734  0.781422  0.154361  0.303809   \n",
       "...        ...       ...   ...  ...       ...       ...       ...       ...   \n",
       "3746  0.033300  0.506409  0.10  0.0  0.209887  0.633426  0.297659  0.376124   \n",
       "3747  0.133333  0.651023  0.15  0.0  0.151154  0.766505  0.170876  0.404546   \n",
       "3748  0.200000  0.520564  0.00  0.0  0.179949  0.768785  0.177341  0.471179   \n",
       "3749  0.100000  0.765646  0.00  0.0  0.536954  0.634936  0.342713  0.447162   \n",
       "3750  0.133333  0.533952  0.00  0.0  0.347966  0.757971  0.230667  0.272652   \n",
       "\n",
       "            D9       D10  ...  D1767  D1768  D1769  D1770  D1771  D1772  \\\n",
       "0     0.743663  0.243144  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1     0.836582  0.106480  ...    1.0    1.0    1.0    1.0    0.0    1.0   \n",
       "2     0.679051  0.352308  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3     0.805110  0.208989  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4     0.812646  0.125177  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...        ...       ...  ...    ...    ...    ...    ...    ...    ...   \n",
       "3746  0.727093  0.308163  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3747  0.787935  0.192527  ...    0.0    0.0    1.0    0.0    1.0    0.0   \n",
       "3748  0.872241  0.122132  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3749  0.672689  0.372936  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3750  0.854116  0.140316  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "      D1773  D1774  D1775  D1776  \n",
       "0       0.0    0.0    0.0    0.0  \n",
       "1       0.0    0.0    1.0    0.0  \n",
       "2       0.0    0.0    0.0    0.0  \n",
       "3       0.0    0.0    0.0    0.0  \n",
       "4       0.0    0.0    0.0    0.0  \n",
       "...     ...    ...    ...    ...  \n",
       "3746    0.0    0.0    0.0    0.0  \n",
       "3747    1.0    0.0    0.0    0.0  \n",
       "3748    0.0    0.0    0.0    0.0  \n",
       "3749    0.0    0.0    0.0    0.0  \n",
       "3750    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[3751 rows x 1776 columns]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "564    0\n",
       "565    0\n",
       "566    0\n",
       "567    0\n",
       "568    1\n",
       "Name: target, Length: 569, dtype: int32"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = data[0]\n",
    "target = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['malignant', 'benign']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.target_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель и её обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1776"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = len(feature.keys())\n",
    "b_model = baseline(dims)\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = torch.optim.Adam(b_model.parameters(), lr=1e-4, eps=1e-4)\n",
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.4970, 0.1000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tensors = [torch.tensor(feature[key].values) for key in feature.keys()]\n",
    "grid_tensor = torch.stack(grid_tensors)\n",
    "grid_flattened = grid_tensor.view(grid_tensor.shape[0], -1).transpose(0, 1)\n",
    "grid_flattened = grid_flattened.to(grid_flattened.to(torch.float64))\n",
    "grid_flattened[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = grid_flattened\n",
    "train_target = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2960"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = len(target) // 100 * 80\n",
    "train_features = grid_flattened[:param]\n",
    "train_target = target[:param]\n",
    "test_features = grid_flattened[param:]\n",
    "test_target = target[param:]\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1776, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=256, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (9): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device('cuda')\n",
    "b_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=object)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = torch.tensor(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=1, loss=0.6940646506482357\n",
      "Surface training t=2, loss=0.6936977014113002\n",
      "Surface training t=3, loss=0.693337597936442\n",
      "Surface training t=4, loss=0.6930134513134301\n",
      "Surface training t=5, loss=0.6926827841417923\n",
      "Surface training t=6, loss=0.6924673103500043\n",
      "Surface training t=7, loss=0.6920361970561779\n",
      "Surface training t=8, loss=0.6916319341136166\n",
      "Surface training t=9, loss=0.6913881341422448\n",
      "Surface training t=10, loss=0.6910532290373002\n",
      "Surface training t=11, loss=0.6909146049292536\n",
      "Surface training t=12, loss=0.6905733458329515\n",
      "Surface training t=13, loss=0.6900756929892894\n",
      "Surface training t=14, loss=0.6898373407139831\n",
      "Surface training t=15, loss=0.6899781314065478\n",
      "Surface training t=16, loss=0.6896700563161657\n",
      "Surface training t=17, loss=0.6894660255045102\n",
      "Surface training t=18, loss=0.6878942639999683\n",
      "Surface training t=19, loss=0.6883182393827744\n",
      "Surface training t=20, loss=0.6882718492945337\n",
      "Surface training t=21, loss=0.6873332926218523\n",
      "Surface training t=22, loss=0.6879189559947407\n",
      "Surface training t=23, loss=0.6868051380784896\n",
      "Surface training t=24, loss=0.6864032650360765\n",
      "Surface training t=25, loss=0.6867864329878337\n",
      "Surface training t=26, loss=0.6856328365099289\n",
      "Surface training t=27, loss=0.6843899256431671\n",
      "Surface training t=28, loss=0.6849415488753503\n",
      "Surface training t=29, loss=0.6829545130950604\n",
      "Surface training t=30, loss=0.6841357541840011\n",
      "Surface training t=31, loss=0.6836438690121227\n",
      "Surface training t=32, loss=0.6824548611377574\n",
      "Surface training t=33, loss=0.6820851205905833\n",
      "Surface training t=34, loss=0.6823595346478102\n",
      "Surface training t=35, loss=0.6813011627506271\n",
      "Surface training t=36, loss=0.6810812994558608\n",
      "Surface training t=37, loss=0.6791188106496118\n",
      "Surface training t=38, loss=0.6795033101227773\n",
      "Surface training t=39, loss=0.6776391107475268\n",
      "Surface training t=40, loss=0.6775352872816744\n",
      "Surface training t=41, loss=0.6761647803802981\n",
      "Surface training t=42, loss=0.674726360543604\n",
      "Surface training t=43, loss=0.6747413046555253\n",
      "Surface training t=44, loss=0.6729654536288278\n",
      "Surface training t=45, loss=0.6731358810204449\n",
      "Surface training t=46, loss=0.6692248426997216\n",
      "Surface training t=47, loss=0.6681935765533812\n",
      "Surface training t=48, loss=0.6686828255430288\n",
      "Surface training t=49, loss=0.6650310411063316\n",
      "Surface training t=50, loss=0.6644845657259615\n",
      "Surface training t=51, loss=0.6621910804634552\n",
      "Surface training t=52, loss=0.6598986814469006\n",
      "Surface training t=53, loss=0.6594432087171871\n",
      "Surface training t=54, loss=0.6567887139695787\n",
      "Surface training t=55, loss=0.6535502322540421\n",
      "Surface training t=56, loss=0.6543044375674272\n",
      "Surface training t=57, loss=0.6480327958906136\n",
      "Surface training t=58, loss=0.6473290859525653\n",
      "Surface training t=59, loss=0.6429191279466935\n",
      "Surface training t=60, loss=0.6411665548104799\n",
      "Surface training t=61, loss=0.6385490031332812\n",
      "Surface training t=62, loss=0.6351997030236858\n",
      "Surface training t=63, loss=0.629113848737784\n",
      "Surface training t=64, loss=0.6286086601347809\n",
      "Surface training t=65, loss=0.6240033734433035\n",
      "Surface training t=66, loss=0.6181733675307555\n",
      "Surface training t=67, loss=0.6137077125440076\n",
      "Surface training t=68, loss=0.6120987034551647\n",
      "Surface training t=69, loss=0.6053782888064105\n",
      "Surface training t=70, loss=0.6041698191509575\n",
      "Surface training t=71, loss=0.5965492231332417\n",
      "Surface training t=72, loss=0.5934962162872699\n",
      "Surface training t=73, loss=0.5879974773109171\n",
      "Surface training t=74, loss=0.5857241896108014\n",
      "Surface training t=75, loss=0.5781403502298135\n",
      "Surface training t=76, loss=0.5742632233986305\n",
      "Surface training t=77, loss=0.5703366808212554\n",
      "Surface training t=78, loss=0.5644273441779951\n",
      "Surface training t=79, loss=0.5600615691002572\n",
      "Surface training t=80, loss=0.5525380243673716\n",
      "Surface training t=81, loss=0.5559211683065062\n",
      "Surface training t=82, loss=0.5472567484349957\n",
      "Surface training t=83, loss=0.5377481312735672\n",
      "Surface training t=84, loss=0.5371790773749736\n",
      "Surface training t=85, loss=0.5295521991969661\n",
      "Surface training t=86, loss=0.5256883653743736\n",
      "Surface training t=87, loss=0.5200601221289667\n",
      "Surface training t=88, loss=0.5167469913807712\n",
      "Surface training t=89, loss=0.507802749865243\n",
      "Surface training t=90, loss=0.5043992104746171\n",
      "Surface training t=91, loss=0.5001942486098587\n",
      "Surface training t=92, loss=0.49726317879896875\n",
      "Surface training t=93, loss=0.49195501526917895\n",
      "Surface training t=94, loss=0.4864729924481527\n",
      "Surface training t=95, loss=0.4835333151674629\n",
      "Surface training t=96, loss=0.48309819442065616\n",
      "Surface training t=97, loss=0.47022360186166134\n",
      "Surface training t=98, loss=0.4686437670189877\n",
      "Surface training t=99, loss=0.46338345845518325\n",
      "Surface training t=100, loss=0.46596536734288624\n",
      "Surface training t=101, loss=0.4525636636168292\n",
      "Surface training t=102, loss=0.4532420872306201\n",
      "Surface training t=103, loss=0.4497894371394676\n",
      "Surface training t=104, loss=0.4411705986921813\n",
      "Surface training t=105, loss=0.4399700348088555\n",
      "Surface training t=106, loss=0.43053676069715063\n",
      "Surface training t=107, loss=0.43043227879677726\n",
      "Surface training t=108, loss=0.43543929656569946\n",
      "Surface training t=109, loss=0.431922143852839\n",
      "Surface training t=110, loss=0.4240681800493423\n",
      "Surface training t=111, loss=0.4189496471469312\n",
      "Surface training t=112, loss=0.41396858983460805\n",
      "Surface training t=113, loss=0.4134618371053693\n",
      "Surface training t=114, loss=0.40961317795552266\n",
      "Surface training t=115, loss=0.40430026138671504\n",
      "Surface training t=116, loss=0.3997205935028107\n",
      "Surface training t=117, loss=0.39861716758717836\n",
      "Surface training t=118, loss=0.39720981621680246\n",
      "Surface training t=119, loss=0.39726715452830996\n",
      "Surface training t=120, loss=0.3940870772265226\n",
      "Surface training t=121, loss=0.3900579262987828\n",
      "Surface training t=122, loss=0.3910067982836686\n",
      "Surface training t=123, loss=0.38106608413892135\n",
      "Surface training t=124, loss=0.38816981105908316\n",
      "Surface training t=125, loss=0.3821277759753182\n",
      "Surface training t=126, loss=0.3832622967342765\n",
      "Surface training t=127, loss=0.38533310707927165\n",
      "Surface training t=128, loss=0.37557261262453107\n",
      "Surface training t=129, loss=0.3686453084846958\n",
      "Surface training t=130, loss=0.3741569044020382\n",
      "Surface training t=131, loss=0.36815717352508376\n",
      "Surface training t=132, loss=0.3707473590697976\n",
      "Surface training t=133, loss=0.3705798681445427\n",
      "Surface training t=134, loss=0.3604722469427921\n",
      "Surface training t=135, loss=0.35948689452741645\n",
      "Surface training t=136, loss=0.35932142332710915\n",
      "Surface training t=137, loss=0.3564918070487274\n",
      "Surface training t=138, loss=0.35471470241615677\n",
      "Surface training t=139, loss=0.3530203258322802\n",
      "Surface training t=140, loss=0.3457480144632258\n",
      "Surface training t=141, loss=0.3483234701425163\n",
      "Surface training t=142, loss=0.34961995729297224\n",
      "Surface training t=143, loss=0.3492175692251248\n",
      "Surface training t=144, loss=0.3436500576106718\n",
      "Surface training t=145, loss=0.33971715557549353\n",
      "Surface training t=146, loss=0.3361275812539759\n",
      "Surface training t=147, loss=0.3400408297168809\n",
      "Surface training t=148, loss=0.3358601633118421\n",
      "Surface training t=149, loss=0.33663981697191947\n",
      "Surface training t=150, loss=0.3344274276744956\n",
      "Surface training t=151, loss=0.32846214612161706\n",
      "Surface training t=152, loss=0.32726217335433383\n",
      "Surface training t=153, loss=0.32036492940350647\n",
      "Surface training t=154, loss=0.3227592700569103\n",
      "Surface training t=155, loss=0.3211609165950158\n",
      "Surface training t=156, loss=0.3173416221965767\n",
      "Surface training t=157, loss=0.3216079313071334\n",
      "Surface training t=158, loss=0.3104106538082951\n",
      "Surface training t=159, loss=0.31662192621071095\n",
      "Surface training t=160, loss=0.3164620592061891\n",
      "Surface training t=161, loss=0.3099743298225226\n",
      "Surface training t=162, loss=0.31676453563998663\n",
      "Surface training t=163, loss=0.30785756092704764\n",
      "Surface training t=164, loss=0.3075340118694921\n",
      "Surface training t=165, loss=0.30484545984976136\n",
      "Surface training t=166, loss=0.30602844639713594\n",
      "Surface training t=167, loss=0.29773720240290735\n",
      "Surface training t=168, loss=0.3003924289778397\n",
      "Surface training t=169, loss=0.3008728536986551\n",
      "Surface training t=170, loss=0.29502897092309516\n",
      "Surface training t=171, loss=0.29640324868673273\n",
      "Surface training t=172, loss=0.2900767375869898\n",
      "Surface training t=173, loss=0.29069671593373864\n",
      "Surface training t=174, loss=0.287311455422806\n",
      "Surface training t=175, loss=0.2858764311186012\n",
      "Surface training t=176, loss=0.2820755401944509\n",
      "Surface training t=177, loss=0.2749307415706511\n",
      "Surface training t=178, loss=0.2829351147954666\n",
      "Surface training t=179, loss=0.2812361986578059\n",
      "Surface training t=180, loss=0.27323178273168114\n",
      "Surface training t=181, loss=0.26894642056458296\n",
      "Surface training t=182, loss=0.2718009199487636\n",
      "Surface training t=183, loss=0.2655374498748211\n",
      "Surface training t=184, loss=0.26538526223167\n",
      "Surface training t=185, loss=0.26149886428771163\n",
      "Surface training t=186, loss=0.26665118395696574\n",
      "Surface training t=187, loss=0.2607277289655745\n",
      "Surface training t=188, loss=0.25700474174775656\n",
      "Surface training t=189, loss=0.2540902200196532\n",
      "Surface training t=190, loss=0.25384555399580233\n",
      "Surface training t=191, loss=0.2517149090926267\n",
      "Surface training t=192, loss=0.24930240405392035\n",
      "Surface training t=193, loss=0.2513899171609544\n",
      "Surface training t=194, loss=0.24165092021966905\n",
      "Surface training t=195, loss=0.24860510829605081\n",
      "Surface training t=196, loss=0.24341279337570926\n",
      "Surface training t=197, loss=0.23586828951700012\n",
      "Surface training t=198, loss=0.234801139361265\n",
      "Surface training t=199, loss=0.2370869736025789\n",
      "Surface training t=200, loss=0.23606046951732654\n",
      "Surface training t=201, loss=0.235805816520182\n",
      "Surface training t=202, loss=0.22749633812007422\n",
      "Surface training t=203, loss=0.23178192974564496\n",
      "Surface training t=204, loss=0.22600346220382964\n",
      "Surface training t=205, loss=0.2289512316942517\n",
      "Surface training t=206, loss=0.21930902821054266\n",
      "Surface training t=207, loss=0.21650657537681134\n",
      "Surface training t=208, loss=0.22369002080250755\n",
      "Surface training t=209, loss=0.21161924089573858\n",
      "Surface training t=210, loss=0.2157221633148332\n",
      "Surface training t=211, loss=0.21206749872423575\n",
      "Surface training t=212, loss=0.20385641975412666\n",
      "Surface training t=213, loss=0.2017254601105561\n",
      "Surface training t=214, loss=0.20554033695576263\n",
      "Surface training t=215, loss=0.19903632250429198\n",
      "Surface training t=216, loss=0.2005017570211461\n",
      "Surface training t=217, loss=0.2008125713914653\n",
      "Surface training t=218, loss=0.19836131972394966\n",
      "Surface training t=219, loss=0.20077853916785743\n",
      "Surface training t=220, loss=0.19276816093496513\n",
      "Surface training t=221, loss=0.19394160227808543\n",
      "Surface training t=222, loss=0.1866503990035322\n",
      "Surface training t=223, loss=0.19088120265285174\n",
      "Surface training t=224, loss=0.18772803373175478\n",
      "Surface training t=225, loss=0.18618409083238868\n",
      "Surface training t=226, loss=0.18388686445454572\n",
      "Surface training t=227, loss=0.18213379137570257\n",
      "Surface training t=228, loss=0.17915316955087932\n",
      "Surface training t=229, loss=0.17693602156484534\n",
      "Surface training t=230, loss=0.17135158520450788\n",
      "Surface training t=231, loss=0.17716977213734636\n",
      "Surface training t=232, loss=0.17757569742533638\n",
      "Surface training t=233, loss=0.17164157034436966\n",
      "Surface training t=234, loss=0.16386873619079184\n",
      "Surface training t=235, loss=0.17004677032053653\n",
      "Surface training t=236, loss=0.16565075104376503\n",
      "Surface training t=237, loss=0.1641577885845993\n",
      "Surface training t=238, loss=0.16418411602322885\n",
      "Surface training t=239, loss=0.16400969468407656\n",
      "Surface training t=240, loss=0.15756562913514038\n",
      "Surface training t=241, loss=0.1587406862220297\n",
      "Surface training t=242, loss=0.15805446673350537\n",
      "Surface training t=243, loss=0.15538428990913628\n",
      "Surface training t=244, loss=0.15611303399563597\n",
      "Surface training t=245, loss=0.15595985029705872\n",
      "Surface training t=246, loss=0.15057490179012956\n",
      "Surface training t=247, loss=0.15109970359334868\n",
      "Surface training t=248, loss=0.1447865444229051\n",
      "Surface training t=249, loss=0.14701366255106363\n",
      "Surface training t=250, loss=0.1471690920636305\n",
      "Surface training t=251, loss=0.14404398848235522\n",
      "Surface training t=252, loss=0.1422530283029559\n",
      "Surface training t=253, loss=0.13419405051123262\n",
      "Surface training t=254, loss=0.13803257030590854\n",
      "Surface training t=255, loss=0.13899629797549973\n",
      "Surface training t=256, loss=0.13327633636875333\n",
      "Surface training t=257, loss=0.13397765712857895\n",
      "Surface training t=258, loss=0.12833156079482855\n",
      "Surface training t=259, loss=0.13306756551643847\n",
      "Surface training t=260, loss=0.12801202019578442\n",
      "Surface training t=261, loss=0.12412524643209663\n",
      "Surface training t=262, loss=0.12806658735197124\n",
      "Surface training t=263, loss=0.12751128163855363\n",
      "Surface training t=264, loss=0.12667054264088304\n",
      "Surface training t=265, loss=0.1210118598715444\n",
      "Surface training t=266, loss=0.1221996561889249\n",
      "Surface training t=267, loss=0.12043314262770698\n",
      "Surface training t=268, loss=0.12069220465754618\n",
      "Surface training t=269, loss=0.11730545471053071\n",
      "Surface training t=270, loss=0.12002323553713423\n",
      "Surface training t=271, loss=0.11476488242865243\n",
      "Surface training t=272, loss=0.10807451358514478\n",
      "Surface training t=273, loss=0.11173697821626227\n",
      "Surface training t=274, loss=0.1112707308045143\n",
      "Surface training t=275, loss=0.11312685056511257\n",
      "Surface training t=276, loss=0.11220829376405535\n",
      "Surface training t=277, loss=0.10487672330411615\n",
      "Surface training t=278, loss=0.10704746083837005\n",
      "Surface training t=279, loss=0.10340474027448276\n",
      "Surface training t=280, loss=0.10135514926137636\n",
      "Surface training t=281, loss=0.10243526834120395\n",
      "Surface training t=282, loss=0.10218975319898879\n",
      "Surface training t=283, loss=0.09846047365448671\n",
      "Surface training t=284, loss=0.09626893238872206\n",
      "Surface training t=285, loss=0.09630539708003048\n",
      "Surface training t=286, loss=0.09459320604561963\n",
      "Surface training t=287, loss=0.0961727657997774\n",
      "Surface training t=288, loss=0.09091472299503936\n",
      "Surface training t=289, loss=0.09207320592623718\n",
      "Surface training t=290, loss=0.09230483621267595\n",
      "Surface training t=291, loss=0.09004492320584878\n",
      "Surface training t=292, loss=0.08695781601182369\n",
      "Surface training t=293, loss=0.08693870108139948\n",
      "Surface training t=294, loss=0.08844083933357047\n",
      "Surface training t=295, loss=0.08866709821940065\n",
      "Surface training t=296, loss=0.09012556819423376\n",
      "Surface training t=297, loss=0.08602644253311581\n",
      "Surface training t=298, loss=0.08536348703421552\n",
      "Surface training t=299, loss=0.08794189284996151\n",
      "Surface training t=300, loss=0.08229180112350626\n",
      "Surface training t=301, loss=0.08183323977449034\n",
      "Surface training t=302, loss=0.07959350435340645\n",
      "Surface training t=303, loss=0.07881869971119934\n",
      "Surface training t=304, loss=0.0775873310541601\n",
      "Surface training t=305, loss=0.07938901563085866\n",
      "Surface training t=306, loss=0.07826404308236255\n",
      "Surface training t=307, loss=0.07607191146567058\n",
      "Surface training t=308, loss=0.07392973097912503\n",
      "Surface training t=309, loss=0.07252927892023073\n",
      "Surface training t=310, loss=0.07557721436198139\n",
      "Surface training t=311, loss=0.07457235896592432\n",
      "Surface training t=312, loss=0.07205978012584133\n",
      "Surface training t=313, loss=0.07093657199660727\n",
      "Surface training t=314, loss=0.07049437436748501\n",
      "Surface training t=315, loss=0.07011555052620676\n",
      "Surface training t=316, loss=0.06638663703108973\n",
      "Surface training t=317, loss=0.07016910068503666\n",
      "Surface training t=318, loss=0.06679307740032396\n",
      "Surface training t=319, loss=0.06600524156300364\n",
      "Surface training t=320, loss=0.06584700480526803\n",
      "Surface training t=321, loss=0.06657571894312542\n",
      "Surface training t=322, loss=0.06603692998372122\n",
      "Surface training t=323, loss=0.06405314871395561\n",
      "Surface training t=324, loss=0.06449841115598766\n",
      "Surface training t=325, loss=0.06088510004904736\n",
      "Surface training t=326, loss=0.0623393307242012\n",
      "Surface training t=327, loss=0.059542250210826174\n",
      "Surface training t=328, loss=0.05974865606620926\n",
      "Surface training t=329, loss=0.05940458478836598\n",
      "Surface training t=330, loss=0.058479152072353845\n",
      "Surface training t=331, loss=0.056420825056066395\n",
      "Surface training t=332, loss=0.05960798747584145\n",
      "Surface training t=333, loss=0.0559845425817467\n",
      "Surface training t=334, loss=0.05556717941400735\n",
      "Surface training t=335, loss=0.056329747535418064\n",
      "Surface training t=336, loss=0.05520648055845877\n",
      "Surface training t=337, loss=0.05514345789106027\n",
      "Surface training t=338, loss=0.05632235138090398\n",
      "Surface training t=339, loss=0.053560023829693645\n",
      "Surface training t=340, loss=0.05133112713715847\n",
      "Surface training t=341, loss=0.05240756379971445\n",
      "Surface training t=342, loss=0.054319022648990536\n",
      "Surface training t=343, loss=0.04953626174087422\n",
      "Surface training t=344, loss=0.04999162436592637\n",
      "Surface training t=345, loss=0.05062197770525294\n",
      "Surface training t=346, loss=0.05265485555036166\n",
      "Surface training t=347, loss=0.04981971909543892\n",
      "Surface training t=348, loss=0.04855622992397508\n",
      "Surface training t=349, loss=0.046829856068773136\n",
      "Surface training t=350, loss=0.050718775373073316\n",
      "Surface training t=351, loss=0.04680489493412483\n",
      "Surface training t=352, loss=0.04725117102276461\n",
      "Surface training t=353, loss=0.045909564405261144\n",
      "Surface training t=354, loss=0.04478439510015858\n",
      "Surface training t=355, loss=0.04424430093869092\n",
      "Surface training t=356, loss=0.046191681391219944\n",
      "Surface training t=357, loss=0.04457515483561829\n",
      "Surface training t=358, loss=0.04380271540354106\n",
      "Surface training t=359, loss=0.04283664259383298\n",
      "Surface training t=360, loss=0.04277849904380654\n",
      "Surface training t=361, loss=0.04190357590306254\n",
      "Surface training t=362, loss=0.03919040916969912\n",
      "Surface training t=363, loss=0.04342893979661186\n",
      "Surface training t=364, loss=0.03924688464215523\n",
      "Surface training t=365, loss=0.04268190753099837\n",
      "Surface training t=366, loss=0.04108164358236312\n",
      "Surface training t=367, loss=0.03964502725761257\n",
      "Surface training t=368, loss=0.041109797614387246\n",
      "Surface training t=369, loss=0.0399272980145052\n",
      "Surface training t=370, loss=0.03879300846175015\n",
      "Surface training t=371, loss=0.04159393735757279\n",
      "Surface training t=372, loss=0.03861993685109863\n",
      "Surface training t=373, loss=0.039452675493659425\n",
      "Surface training t=374, loss=0.03919643592859472\n",
      "Surface training t=375, loss=0.037350618224433536\n",
      "Surface training t=376, loss=0.03750819669572314\n",
      "Surface training t=377, loss=0.03773111408533107\n",
      "Surface training t=378, loss=0.03502028041212944\n",
      "Surface training t=379, loss=0.036549540217162425\n",
      "Surface training t=380, loss=0.03679466134715456\n",
      "Surface training t=381, loss=0.03481919886635987\n",
      "Surface training t=382, loss=0.034134767003255295\n",
      "Surface training t=383, loss=0.034699478736060066\n",
      "Surface training t=384, loss=0.03394921898597658\n",
      "Surface training t=385, loss=0.03314580801008925\n",
      "Surface training t=386, loss=0.033624755780613344\n",
      "Surface training t=387, loss=0.03276709263250468\n",
      "Surface training t=388, loss=0.03078962788540991\n",
      "Surface training t=389, loss=0.032986864446672655\n",
      "Surface training t=390, loss=0.03210751999144883\n",
      "Surface training t=391, loss=0.03036194392936606\n",
      "Surface training t=392, loss=0.0318823033751625\n",
      "Surface training t=393, loss=0.030482662147822408\n",
      "Surface training t=394, loss=0.030749139007584254\n",
      "Surface training t=395, loss=0.030178160463731306\n",
      "Surface training t=396, loss=0.029790339032199773\n",
      "Surface training t=397, loss=0.028389155610856632\n",
      "Surface training t=398, loss=0.02848588722583715\n",
      "Surface training t=399, loss=0.030182032714749132\n",
      "Surface training t=400, loss=0.027731887287162204\n",
      "Surface training t=401, loss=0.028665088212788035\n",
      "Surface training t=402, loss=0.02928874840709617\n",
      "Surface training t=403, loss=0.02654990199775173\n",
      "Surface training t=404, loss=0.027791298306150777\n",
      "Surface training t=405, loss=0.029152648175669087\n",
      "Surface training t=406, loss=0.0281664747165183\n",
      "Surface training t=407, loss=0.027432874940434913\n",
      "Surface training t=408, loss=0.026436093378072734\n",
      "Surface training t=409, loss=0.027762275353688037\n",
      "Surface training t=410, loss=0.026336786832800156\n",
      "Surface training t=411, loss=0.02741024922392414\n",
      "Surface training t=412, loss=0.025436985848282773\n",
      "Surface training t=413, loss=0.02646798979861118\n",
      "Surface training t=414, loss=0.025038784021041795\n",
      "Surface training t=415, loss=0.025520074646563355\n",
      "Surface training t=416, loss=0.02436771635889126\n",
      "Surface training t=417, loss=0.023927723881567563\n",
      "Surface training t=418, loss=0.02472760817803311\n",
      "Surface training t=419, loss=0.024460698717200545\n",
      "Surface training t=420, loss=0.02415249329641856\n",
      "Surface training t=421, loss=0.024783088339824663\n",
      "Surface training t=422, loss=0.025350331937482873\n",
      "Surface training t=423, loss=0.024871043892340676\n",
      "Surface training t=424, loss=0.02480035664586249\n",
      "Surface training t=425, loss=0.02426543667645402\n",
      "Surface training t=426, loss=0.023708735877820502\n",
      "Surface training t=427, loss=0.02392560188450482\n",
      "Surface training t=428, loss=0.023262399737702166\n",
      "Surface training t=429, loss=0.02444779817728299\n",
      "Surface training t=430, loss=0.024499291183469298\n",
      "Surface training t=431, loss=0.023318066678551887\n",
      "Surface training t=432, loss=0.02477794814189517\n",
      "Surface training t=433, loss=0.02313138937657153\n",
      "Surface training t=434, loss=0.02310188118253556\n",
      "Surface training t=435, loss=0.022619019214821758\n",
      "Surface training t=436, loss=0.02108797660523416\n",
      "Surface training t=437, loss=0.020941278583392363\n",
      "Surface training t=438, loss=0.022106799675295268\n",
      "Surface training t=439, loss=0.021369696916618365\n",
      "Surface training t=440, loss=0.020244245443131456\n",
      "Surface training t=441, loss=0.020193782895942668\n",
      "Surface training t=442, loss=0.019998736559376413\n",
      "Surface training t=443, loss=0.021225986227724977\n",
      "Surface training t=444, loss=0.019693916621578655\n",
      "Surface training t=445, loss=0.019838865201472128\n",
      "Surface training t=446, loss=0.018727955721272112\n",
      "Surface training t=447, loss=0.01935787416088988\n",
      "Surface training t=448, loss=0.019487010375781315\n",
      "Surface training t=449, loss=0.018772636700870895\n",
      "Surface training t=450, loss=0.020260082496274954\n",
      "Surface training t=451, loss=0.019903933585127438\n",
      "Surface training t=452, loss=0.018871455828373837\n",
      "Surface training t=453, loss=0.01886720477364115\n",
      "Surface training t=454, loss=0.01981439366350839\n",
      "Surface training t=455, loss=0.019020850399532538\n",
      "Surface training t=456, loss=0.019851549734674977\n",
      "Surface training t=457, loss=0.01852671422781178\n",
      "Surface training t=458, loss=0.019359887725515744\n",
      "Surface training t=459, loss=0.018781267614585075\n",
      "Surface training t=460, loss=0.017373066750520257\n",
      "Surface training t=461, loss=0.018012600361889904\n",
      "Surface training t=462, loss=0.01699432911521924\n",
      "Surface training t=463, loss=0.01800932934136467\n",
      "Surface training t=464, loss=0.016925847427854614\n",
      "Surface training t=465, loss=0.017493125801451244\n",
      "Surface training t=466, loss=0.017916942945360125\n",
      "Surface training t=467, loss=0.016620093291142182\n",
      "Surface training t=468, loss=0.016006450738921454\n",
      "Surface training t=469, loss=0.0182321734638502\n",
      "Surface training t=470, loss=0.01640243793751218\n",
      "Surface training t=471, loss=0.016161903816315774\n",
      "Surface training t=472, loss=0.015431772498026906\n",
      "Surface training t=473, loss=0.01594855274098051\n",
      "Surface training t=474, loss=0.016099858286854955\n",
      "Surface training t=475, loss=0.015046044100962577\n",
      "Surface training t=476, loss=0.015672595702908885\n",
      "Surface training t=477, loss=0.014875285096771121\n",
      "Surface training t=478, loss=0.017497293585968677\n",
      "Surface training t=479, loss=0.016252604563689665\n",
      "Surface training t=480, loss=0.014696059239329293\n",
      "Surface training t=481, loss=0.015333970610067284\n",
      "Surface training t=482, loss=0.015526722548744527\n",
      "Surface training t=483, loss=0.016178845976110124\n",
      "Surface training t=484, loss=0.01422854200938929\n",
      "Surface training t=485, loss=0.014614534771065904\n",
      "Surface training t=486, loss=0.013741337832975889\n",
      "Surface training t=487, loss=0.013912970836979836\n",
      "Surface training t=488, loss=0.014216054374691436\n",
      "Surface training t=489, loss=0.014435250796685992\n",
      "Surface training t=490, loss=0.013448991734584657\n",
      "Surface training t=491, loss=0.01413173368316015\n",
      "Surface training t=492, loss=0.012959664835822959\n",
      "Surface training t=493, loss=0.013057984998979547\n",
      "Surface training t=494, loss=0.013896521902238435\n",
      "Surface training t=495, loss=0.013458673566000601\n",
      "Surface training t=496, loss=0.014430574929641426\n",
      "Surface training t=497, loss=0.012720848421925116\n",
      "Surface training t=498, loss=0.013225499636871984\n",
      "Surface training t=499, loss=0.012802949597002393\n",
      "Surface training t=500, loss=0.012689935665654573\n",
      "Surface training t=501, loss=0.01233980090014447\n",
      "Surface training t=502, loss=0.013261103756691666\n",
      "Surface training t=503, loss=0.011973314170386572\n",
      "Surface training t=504, loss=0.012066682116478347\n",
      "Surface training t=505, loss=0.011336689462993575\n",
      "Surface training t=506, loss=0.012257426940594045\n",
      "Surface training t=507, loss=0.012367686540665166\n",
      "Surface training t=508, loss=0.012004034335485197\n",
      "Surface training t=509, loss=0.011444368543766882\n",
      "Surface training t=510, loss=0.01168165249756324\n",
      "Surface training t=511, loss=0.010869811588168906\n",
      "Surface training t=512, loss=0.012145810957941476\n",
      "Surface training t=513, loss=0.011233041838594936\n",
      "Surface training t=514, loss=0.011162100156401128\n",
      "Surface training t=515, loss=0.011519714638012449\n",
      "Surface training t=516, loss=0.010865789417606658\n",
      "Surface training t=517, loss=0.011608058936157878\n",
      "Surface training t=518, loss=0.010745079729501603\n",
      "Surface training t=519, loss=0.010448863952865473\n",
      "Surface training t=520, loss=0.011548095946833623\n",
      "Surface training t=521, loss=0.01040028147805038\n",
      "Surface training t=522, loss=0.010545916086663975\n",
      "Surface training t=523, loss=0.010276139479434785\n",
      "Surface training t=524, loss=0.009825625305544997\n",
      "Surface training t=525, loss=0.009713006937494578\n",
      "Surface training t=526, loss=0.010441942296562402\n",
      "Surface training t=527, loss=0.01013973011270959\n",
      "Surface training t=528, loss=0.010155146385025045\n",
      "Surface training t=529, loss=0.01009111408108619\n",
      "Surface training t=530, loss=0.010438182122755237\n",
      "Surface training t=531, loss=0.009833698982411411\n",
      "Surface training t=532, loss=0.009895252019447152\n",
      "Surface training t=533, loss=0.00892275791464969\n",
      "Surface training t=534, loss=0.009774024232294695\n",
      "Surface training t=535, loss=0.009548892228552801\n",
      "Surface training t=536, loss=0.009642757489233055\n",
      "Surface training t=537, loss=0.008872332438917049\n",
      "Surface training t=538, loss=0.009814218984475247\n",
      "Surface training t=539, loss=0.009878579971411453\n",
      "Surface training t=540, loss=0.00912806223949712\n",
      "Surface training t=541, loss=0.009575861363702348\n",
      "Surface training t=542, loss=0.011061548697464698\n",
      "Surface training t=543, loss=0.009435397611575913\n",
      "Surface training t=544, loss=0.00911546535233106\n",
      "Surface training t=545, loss=0.009271567853246504\n",
      "Surface training t=546, loss=0.00990948092025183\n",
      "Surface training t=547, loss=0.008451759779445324\n",
      "Surface training t=548, loss=0.009616685113897943\n",
      "Surface training t=549, loss=0.008986013088481147\n",
      "Surface training t=550, loss=0.008972901835824356\n",
      "Surface training t=551, loss=0.008764534229881712\n",
      "Surface training t=552, loss=0.009000314208397638\n",
      "Surface training t=553, loss=0.008907606410044126\n",
      "Surface training t=554, loss=0.008790648397273997\n",
      "Surface training t=555, loss=0.007518726634729017\n",
      "Surface training t=556, loss=0.008139785729205212\n",
      "Surface training t=557, loss=0.008305603556719346\n",
      "Surface training t=558, loss=0.008896100406602221\n",
      "Surface training t=559, loss=0.00806779182877079\n",
      "Surface training t=560, loss=0.007694060691305627\n",
      "Surface training t=561, loss=0.007205529340870421\n",
      "Surface training t=562, loss=0.007554775799397324\n",
      "Surface training t=563, loss=0.00837340056274324\n",
      "Surface training t=564, loss=0.0075843682026641435\n",
      "Surface training t=565, loss=0.00773448559558958\n",
      "Surface training t=566, loss=0.007429420708168094\n",
      "Surface training t=567, loss=0.007571802578818405\n",
      "Surface training t=568, loss=0.007509536891942885\n",
      "Surface training t=569, loss=0.0074380629403644795\n",
      "Surface training t=570, loss=0.007310954738682605\n",
      "Surface training t=571, loss=0.006858768885619102\n",
      "Surface training t=572, loss=0.007208062399918762\n",
      "Surface training t=573, loss=0.007644399541710584\n",
      "Surface training t=574, loss=0.007112370418191906\n",
      "Surface training t=575, loss=0.0072657411484167295\n",
      "Surface training t=576, loss=0.006856169074221083\n",
      "Surface training t=577, loss=0.007198696839724759\n",
      "Surface training t=578, loss=0.007419437946955421\n",
      "Surface training t=579, loss=0.0071484813084341445\n",
      "Surface training t=580, loss=0.00739205334164666\n",
      "Surface training t=581, loss=0.006716002726792044\n",
      "Surface training t=582, loss=0.006634328903423915\n",
      "Surface training t=583, loss=0.006565978603929547\n",
      "Surface training t=584, loss=0.0067462343842614025\n",
      "Surface training t=585, loss=0.006491873274672204\n",
      "Surface training t=586, loss=0.006493007501522829\n",
      "Surface training t=587, loss=0.00644343586866473\n",
      "Surface training t=588, loss=0.006542888496230357\n",
      "Surface training t=589, loss=0.006404973900442836\n",
      "Surface training t=590, loss=0.006432368317155329\n",
      "Surface training t=591, loss=0.006312940779546635\n",
      "Surface training t=592, loss=0.006188224000913637\n",
      "Surface training t=593, loss=0.0064748772647097615\n",
      "Surface training t=594, loss=0.006590145180787237\n",
      "Surface training t=595, loss=0.005823930945328577\n",
      "Surface training t=596, loss=0.006456720006064174\n",
      "Surface training t=597, loss=0.00643868229070149\n",
      "Surface training t=598, loss=0.005887509101803595\n",
      "Surface training t=599, loss=0.006577847688750961\n",
      "Surface training t=600, loss=0.006169199388129477\n",
      "Surface training t=601, loss=0.006053681511563894\n",
      "Surface training t=602, loss=0.006980969455369348\n",
      "Surface training t=603, loss=0.00602353720964045\n",
      "Surface training t=604, loss=0.005859524197984606\n",
      "Surface training t=605, loss=0.00636689446548906\n",
      "Surface training t=606, loss=0.006646409420440378\n",
      "Surface training t=607, loss=0.005818424713886012\n",
      "Surface training t=608, loss=0.005970343598680436\n",
      "Surface training t=609, loss=0.0059654598289112255\n",
      "Surface training t=610, loss=0.005977213949901914\n",
      "Surface training t=611, loss=0.005771643148399333\n",
      "Surface training t=612, loss=0.005572082931783858\n",
      "Surface training t=613, loss=0.005608825441830341\n",
      "Surface training t=614, loss=0.0055530768646721565\n",
      "Surface training t=615, loss=0.005608874845432581\n",
      "Surface training t=616, loss=0.0055800895872273135\n",
      "Surface training t=617, loss=0.005370613611410403\n",
      "Surface training t=618, loss=0.005804232279150289\n",
      "Surface training t=619, loss=0.00513818165204377\n",
      "Surface training t=620, loss=0.005540185314066384\n",
      "Surface training t=621, loss=0.005395199878395801\n",
      "Surface training t=622, loss=0.00553390431731705\n",
      "Surface training t=623, loss=0.005155879676555814\n",
      "Surface training t=624, loss=0.0050439048544689835\n",
      "Surface training t=625, loss=0.00483646731782717\n",
      "Surface training t=626, loss=0.005354400921649821\n",
      "Surface training t=627, loss=0.005117916374781749\n",
      "Surface training t=628, loss=0.005078640344386789\n",
      "Surface training t=629, loss=0.005008012769493543\n",
      "Surface training t=630, loss=0.004902447444641571\n",
      "Surface training t=631, loss=0.00518428441247268\n",
      "Surface training t=632, loss=0.004828083779511258\n",
      "Surface training t=633, loss=0.0050371381786938375\n",
      "Surface training t=634, loss=0.004784799941925183\n",
      "Surface training t=635, loss=0.00478672974272049\n",
      "Surface training t=636, loss=0.0045730324823036\n",
      "Surface training t=637, loss=0.004735121115561989\n",
      "Surface training t=638, loss=0.0048277194178005176\n",
      "Surface training t=639, loss=0.0052571793252005235\n",
      "Surface training t=640, loss=0.004702176890272743\n",
      "Surface training t=641, loss=0.004507956086358761\n",
      "Surface training t=642, loss=0.004583527142127267\n",
      "Surface training t=643, loss=0.004532953130085558\n",
      "Surface training t=644, loss=0.004778413288872346\n",
      "Surface training t=645, loss=0.0045381534365651725\n",
      "Surface training t=646, loss=0.005022074456816179\n",
      "Surface training t=647, loss=0.004487183734005308\n",
      "Surface training t=648, loss=0.004532598577235794\n",
      "Surface training t=649, loss=0.004330291095597632\n",
      "Surface training t=650, loss=0.004474692377001885\n",
      "Surface training t=651, loss=0.004539308149020761\n",
      "Surface training t=652, loss=0.00468211622315014\n",
      "Surface training t=653, loss=0.004432732302066672\n",
      "Surface training t=654, loss=0.004756604301843392\n",
      "Surface training t=655, loss=0.004133151247418381\n",
      "Surface training t=656, loss=0.0038444454357384125\n",
      "Surface training t=657, loss=0.004689308366745577\n",
      "Surface training t=658, loss=0.004581259433839573\n",
      "Surface training t=659, loss=0.004222043777390282\n",
      "Surface training t=660, loss=0.004237683464067837\n",
      "Surface training t=661, loss=0.004091519858415892\n",
      "Surface training t=662, loss=0.004160239578949906\n",
      "Surface training t=663, loss=0.004188960464023646\n",
      "Surface training t=664, loss=0.004111275645356162\n",
      "Surface training t=665, loss=0.003951492724410868\n",
      "Surface training t=666, loss=0.004229215996350979\n",
      "Surface training t=667, loss=0.0040793618831208005\n",
      "Surface training t=668, loss=0.004319066383551328\n",
      "Surface training t=669, loss=0.004047879896180844\n",
      "Surface training t=670, loss=0.004058417081332659\n",
      "Surface training t=671, loss=0.004180106385009539\n",
      "Surface training t=672, loss=0.004463864501990225\n",
      "Surface training t=673, loss=0.004060103962508061\n",
      "Surface training t=674, loss=0.0041464898652246495\n",
      "Surface training t=675, loss=0.003913668015227241\n",
      "Surface training t=676, loss=0.0038820443808051156\n",
      "Surface training t=677, loss=0.003709642051565253\n",
      "Surface training t=678, loss=0.0038264196763824862\n",
      "Surface training t=679, loss=0.0036780663043501664\n",
      "Surface training t=680, loss=0.0038164193883021566\n",
      "Surface training t=681, loss=0.003775197977611718\n",
      "Surface training t=682, loss=0.004041951830045259\n",
      "Surface training t=683, loss=0.0036955263761864494\n",
      "Surface training t=684, loss=0.003944250475100111\n",
      "Surface training t=685, loss=0.003751467329541199\n",
      "Surface training t=686, loss=0.003671749336645771\n",
      "Surface training t=687, loss=0.003732124620469935\n",
      "Surface training t=688, loss=0.003587874427295009\n",
      "Surface training t=689, loss=0.003847004448445754\n",
      "Surface training t=690, loss=0.003334854075345013\n",
      "Surface training t=691, loss=0.0035203701637357247\n",
      "Surface training t=692, loss=0.003466617656381122\n",
      "Surface training t=693, loss=0.0036180635505086186\n",
      "Surface training t=694, loss=0.003574126235552033\n",
      "Surface training t=695, loss=0.003308753682777432\n",
      "Surface training t=696, loss=0.00356528792016422\n",
      "Surface training t=697, loss=0.0034059820801762364\n",
      "Surface training t=698, loss=0.0033979318638178643\n",
      "Surface training t=699, loss=0.0036023781866094044\n",
      "Surface training t=700, loss=0.003528830909388868\n",
      "Surface training t=701, loss=0.003343935617038633\n",
      "Surface training t=702, loss=0.0034282298175417\n",
      "Surface training t=703, loss=0.0032867015882208535\n",
      "Surface training t=704, loss=0.0035484208990356056\n",
      "Surface training t=705, loss=0.003463068452764421\n",
      "Surface training t=706, loss=0.0032858523469665203\n",
      "Surface training t=707, loss=0.003289006781169292\n",
      "Surface training t=708, loss=0.0034595917423415664\n",
      "Surface training t=709, loss=0.003578386775402534\n",
      "Surface training t=710, loss=0.0030486947787290502\n",
      "Surface training t=711, loss=0.0034942613984762384\n",
      "Surface training t=712, loss=0.0032523645418328683\n",
      "Surface training t=713, loss=0.003668499006870306\n",
      "Surface training t=714, loss=0.00331750444234505\n",
      "Surface training t=715, loss=0.0034484204266912208\n",
      "Surface training t=716, loss=0.003586257674823591\n",
      "Surface training t=717, loss=0.0032078396993880064\n",
      "Surface training t=718, loss=0.003493984336297604\n",
      "Surface training t=719, loss=0.0032265294219669684\n",
      "Surface training t=720, loss=0.0029970664986439465\n",
      "Surface training t=721, loss=0.0033837472957193356\n",
      "Surface training t=722, loss=0.003266480429006543\n",
      "Surface training t=723, loss=0.003313261836782376\n",
      "Surface training t=724, loss=0.0030837277073854016\n",
      "Surface training t=725, loss=0.0032766278241890406\n",
      "Surface training t=726, loss=0.0032722952342479376\n",
      "Surface training t=727, loss=0.0030306247473871\n",
      "Surface training t=728, loss=0.0033921926491168354\n",
      "Surface training t=729, loss=0.003059752059392672\n",
      "Surface training t=730, loss=0.0030436106913944647\n",
      "Surface training t=731, loss=0.00295417306479933\n",
      "Surface training t=732, loss=0.0029441726517202378\n",
      "Surface training t=733, loss=0.003249991920413361\n",
      "Surface training t=734, loss=0.0029446624221123752\n",
      "Surface training t=735, loss=0.0029588800405193497\n",
      "Surface training t=736, loss=0.0030856330927016918\n",
      "Surface training t=737, loss=0.0027697870125937074\n",
      "Surface training t=738, loss=0.003029626752303834\n",
      "Surface training t=739, loss=0.0028276618632870256\n",
      "Surface training t=740, loss=0.0028997079673898168\n",
      "Surface training t=741, loss=0.002656252243409961\n",
      "Surface training t=742, loss=0.003066604388680472\n",
      "Surface training t=743, loss=0.002731534976337961\n",
      "Surface training t=744, loss=0.002984808403599876\n",
      "Surface training t=745, loss=0.0028856015894476905\n",
      "Surface training t=746, loss=0.003064123422025849\n",
      "Surface training t=747, loss=0.0028214177755979623\n",
      "Surface training t=748, loss=0.0030273070810079646\n",
      "Surface training t=749, loss=0.002806457000945465\n",
      "Surface training t=750, loss=0.0025986274535712874\n",
      "Surface training t=751, loss=0.0028165741813016764\n",
      "Surface training t=752, loss=0.0024425923831751135\n",
      "Surface training t=753, loss=0.0028452348073345125\n",
      "Surface training t=754, loss=0.0026897694952469587\n",
      "Surface training t=755, loss=0.002813005979084577\n",
      "Surface training t=756, loss=0.002447738563954392\n",
      "Surface training t=757, loss=0.0025678903210890466\n",
      "Surface training t=758, loss=0.0025391024425359906\n",
      "Surface training t=759, loss=0.0026600042715963557\n",
      "Surface training t=760, loss=0.002605154408357978\n",
      "Surface training t=761, loss=0.002398482523287975\n",
      "Surface training t=762, loss=0.002594462489930712\n",
      "Surface training t=763, loss=0.0028520449530090785\n",
      "Surface training t=764, loss=0.002590131933182295\n",
      "Surface training t=765, loss=0.0024863784351591978\n",
      "Surface training t=766, loss=0.0027127923489041073\n",
      "Surface training t=767, loss=0.0024766889225335924\n",
      "Surface training t=768, loss=0.002294953862902203\n",
      "Surface training t=769, loss=0.0023103411625842334\n",
      "Surface training t=770, loss=0.0024868287646664057\n",
      "Surface training t=771, loss=0.002359001438877872\n",
      "Surface training t=772, loss=0.002461076700740518\n",
      "Surface training t=773, loss=0.002337290955675344\n",
      "Surface training t=774, loss=0.002407303165311048\n",
      "Surface training t=775, loss=0.002305392794271734\n",
      "Surface training t=776, loss=0.0022487270213047148\n",
      "Surface training t=777, loss=0.0021463778305810207\n",
      "Surface training t=778, loss=0.0022146426055163464\n",
      "Surface training t=779, loss=0.002259115867797754\n",
      "Surface training t=780, loss=0.0021587926083092017\n",
      "Surface training t=781, loss=0.002418870487761692\n",
      "Surface training t=782, loss=0.0022594442820556035\n",
      "Surface training t=783, loss=0.0020669130068484308\n",
      "Surface training t=784, loss=0.002315803657762706\n",
      "Surface training t=785, loss=0.002250603476273709\n",
      "Surface training t=786, loss=0.002171873896405758\n",
      "Surface training t=787, loss=0.0021910127960386135\n",
      "Surface training t=788, loss=0.0023077552408336793\n",
      "Surface training t=789, loss=0.0023179780600652558\n",
      "Surface training t=790, loss=0.0020589224084972576\n",
      "Surface training t=791, loss=0.002013826038931515\n",
      "Surface training t=792, loss=0.0021293348516847263\n",
      "Surface training t=793, loss=0.0023526191296704448\n",
      "Surface training t=794, loss=0.002105677614334029\n",
      "Surface training t=795, loss=0.0020951154549084536\n",
      "Surface training t=796, loss=0.001994650473703085\n",
      "Surface training t=797, loss=0.0023051959225682496\n",
      "Surface training t=798, loss=0.002191980936655821\n",
      "Surface training t=799, loss=0.0023076310004824492\n",
      "Surface training t=800, loss=0.0022198706585835606\n",
      "Surface training t=801, loss=0.0022641041156573677\n",
      "Surface training t=802, loss=0.0020983820208211636\n",
      "Surface training t=803, loss=0.002123734880714337\n",
      "Surface training t=804, loss=0.0020774858308306703\n",
      "Surface training t=805, loss=0.0020641267383875745\n",
      "Surface training t=806, loss=0.002053259198040303\n",
      "Surface training t=807, loss=0.0020336836202771973\n",
      "Surface training t=808, loss=0.001875120370962378\n",
      "Surface training t=809, loss=0.002104114120439806\n",
      "Surface training t=810, loss=0.002108457807559248\n",
      "Surface training t=811, loss=0.0020776459746350614\n",
      "Surface training t=812, loss=0.002039670243753196\n",
      "Surface training t=813, loss=0.0020493553456809345\n",
      "Surface training t=814, loss=0.0020433311863995683\n",
      "Surface training t=815, loss=0.00203858750871843\n",
      "Surface training t=816, loss=0.0023793233182312842\n",
      "Surface training t=817, loss=0.0020016132719129264\n",
      "Surface training t=818, loss=0.002009730758696757\n",
      "Surface training t=819, loss=0.0020054118928534894\n",
      "Surface training t=820, loss=0.001975564451437117\n",
      "Surface training t=821, loss=0.0019354538522260711\n",
      "Surface training t=822, loss=0.002017902007303973\n",
      "Surface training t=823, loss=0.001944894545111447\n",
      "Surface training t=824, loss=0.0019361700962389568\n",
      "Surface training t=825, loss=0.0018662462350074408\n",
      "Surface training t=826, loss=0.0018923606931813843\n",
      "Surface training t=827, loss=0.0018199059752149456\n",
      "Surface training t=828, loss=0.001875390157186397\n",
      "Surface training t=829, loss=0.0018999767036743174\n",
      "Surface training t=830, loss=0.0018568262250247527\n",
      "Surface training t=831, loss=0.001936587320427285\n",
      "Surface training t=832, loss=0.0018146162172396362\n",
      "Surface training t=833, loss=0.0017611453887815052\n",
      "Surface training t=834, loss=0.00187645098366642\n",
      "Surface training t=835, loss=0.001995988255858914\n",
      "Surface training t=836, loss=0.0018791229551591664\n",
      "Surface training t=837, loss=0.0019784096657397086\n",
      "Surface training t=838, loss=0.0018614681200864404\n",
      "Surface training t=839, loss=0.0017395364398696562\n",
      "Surface training t=840, loss=0.001764500714765274\n",
      "Surface training t=841, loss=0.0017354935888918414\n",
      "Surface training t=842, loss=0.0017067878203303592\n",
      "Surface training t=843, loss=0.0017352928420468943\n",
      "Surface training t=844, loss=0.001730728249562271\n",
      "Surface training t=845, loss=0.0017914668458911638\n",
      "Surface training t=846, loss=0.001737328605709753\n",
      "Surface training t=847, loss=0.0017378950637056178\n",
      "Surface training t=848, loss=0.0017473963204699158\n",
      "Surface training t=849, loss=0.0017241734375509196\n",
      "Surface training t=850, loss=0.0016481371085558037\n",
      "Surface training t=851, loss=0.0016791034914975084\n",
      "Surface training t=852, loss=0.0017982336267730631\n",
      "Surface training t=853, loss=0.0016378633887811106\n",
      "Surface training t=854, loss=0.0016654563586178\n",
      "Surface training t=855, loss=0.0015915786531445096\n",
      "Surface training t=856, loss=0.0017160076053428287\n",
      "Surface training t=857, loss=0.0017028626205056042\n",
      "Surface training t=858, loss=0.0015423013622347563\n",
      "Surface training t=859, loss=0.0017145713360771746\n",
      "Surface training t=860, loss=0.0016749256390543661\n",
      "Surface training t=861, loss=0.0016016899915460977\n",
      "Surface training t=862, loss=0.0015741474250218594\n",
      "Surface training t=863, loss=0.0017520723321295226\n",
      "Surface training t=864, loss=0.0016720399779346156\n",
      "Surface training t=865, loss=0.0016974175339792375\n",
      "Surface training t=866, loss=0.0017086199399578691\n",
      "Surface training t=867, loss=0.0015694788360100598\n",
      "Surface training t=868, loss=0.001723342569182683\n",
      "Surface training t=869, loss=0.0016197222799086875\n",
      "Surface training t=870, loss=0.0015854596445051875\n",
      "Surface training t=871, loss=0.0015802702718487106\n",
      "Surface training t=872, loss=0.0015136963374815771\n",
      "Surface training t=873, loss=0.0017090057372121748\n",
      "Surface training t=874, loss=0.0016515503399448935\n",
      "Surface training t=875, loss=0.0015825519148078794\n",
      "Surface training t=876, loss=0.0016484325609165689\n",
      "Surface training t=877, loss=0.001553936066153224\n",
      "Surface training t=878, loss=0.0015610967720555704\n",
      "Surface training t=879, loss=0.0016584547640752954\n",
      "Surface training t=880, loss=0.001510887294991505\n",
      "Surface training t=881, loss=0.0014361671580703094\n",
      "Surface training t=882, loss=0.0015024356633867608\n",
      "Surface training t=883, loss=0.0014512332980135168\n",
      "Surface training t=884, loss=0.0016425342895749597\n",
      "Surface training t=885, loss=0.0015171960704757753\n",
      "Surface training t=886, loss=0.0015130981102672364\n",
      "Surface training t=887, loss=0.0014499859784471458\n",
      "Surface training t=888, loss=0.0013914860989414106\n",
      "Surface training t=889, loss=0.001481632901182941\n",
      "Surface training t=890, loss=0.0015478357877602876\n",
      "Surface training t=891, loss=0.001479613438457913\n",
      "Surface training t=892, loss=0.0014068095547029244\n",
      "Surface training t=893, loss=0.001343419344958403\n",
      "Surface training t=894, loss=0.0014083020085967823\n",
      "Surface training t=895, loss=0.0013786624184564675\n",
      "Surface training t=896, loss=0.001387343524895507\n",
      "Surface training t=897, loss=0.0012900892748594077\n",
      "Surface training t=898, loss=0.0013162681558796232\n",
      "Surface training t=899, loss=0.001404668875732736\n",
      "Surface training t=900, loss=0.0013265710949314801\n",
      "Surface training t=901, loss=0.0013965864032642367\n",
      "Surface training t=902, loss=0.001383871652561093\n",
      "Surface training t=903, loss=0.0013980521620851694\n",
      "Surface training t=904, loss=0.0015188497069602886\n",
      "Surface training t=905, loss=0.001379409010975552\n",
      "Surface training t=906, loss=0.0014445277540851183\n",
      "Surface training t=907, loss=0.0014446926892283615\n",
      "Surface training t=908, loss=0.0014079822541723836\n",
      "Surface training t=909, loss=0.0014461644465387576\n",
      "Surface training t=910, loss=0.0013642327880476405\n",
      "Surface training t=911, loss=0.0013889988964585088\n",
      "Surface training t=912, loss=0.0013305164817091293\n",
      "Surface training t=913, loss=0.0013961975662345576\n",
      "Surface training t=914, loss=0.0011952053756269215\n",
      "Surface training t=915, loss=0.0012655298625045678\n",
      "Surface training t=916, loss=0.0013496481206229731\n",
      "Surface training t=917, loss=0.0013323796833809094\n",
      "Surface training t=918, loss=0.0013696432028686562\n",
      "Surface training t=919, loss=0.0013491852700335083\n",
      "Surface training t=920, loss=0.0012876768489278466\n",
      "Surface training t=921, loss=0.0012999280961566461\n",
      "Surface training t=922, loss=0.0012766420419023285\n",
      "Surface training t=923, loss=0.0013778410277196936\n",
      "Surface training t=924, loss=0.0011675356641731715\n",
      "Surface training t=925, loss=0.0013636658938143687\n",
      "Surface training t=926, loss=0.001359180324227399\n",
      "Surface training t=927, loss=0.001353086716065103\n",
      "Surface training t=928, loss=0.0012208071072445387\n",
      "Surface training t=929, loss=0.0012664095507861153\n",
      "Surface training t=930, loss=0.0011932877157604213\n",
      "Surface training t=931, loss=0.0012117978981061146\n",
      "Surface training t=932, loss=0.0013732398695800794\n",
      "Surface training t=933, loss=0.001204635776595552\n",
      "Surface training t=934, loss=0.0012683269895898625\n",
      "Surface training t=935, loss=0.0013149392735105922\n",
      "Surface training t=936, loss=0.0012363490420112826\n",
      "Surface training t=937, loss=0.0012765136449349856\n",
      "Surface training t=938, loss=0.0012012784690747587\n",
      "Surface training t=939, loss=0.0011869647690184088\n",
      "Surface training t=940, loss=0.0011415316009960924\n",
      "Surface training t=941, loss=0.0012519815247640563\n",
      "Surface training t=942, loss=0.0011524165664320313\n",
      "Surface training t=943, loss=0.0011575548274891743\n",
      "Surface training t=944, loss=0.0011847858432206004\n",
      "Surface training t=945, loss=0.0011732913542849487\n",
      "Surface training t=946, loss=0.0011363745369954172\n",
      "Surface training t=947, loss=0.0011813823670730021\n",
      "Surface training t=948, loss=0.001228786597745282\n",
      "Surface training t=949, loss=0.0011575511463393206\n",
      "Surface training t=950, loss=0.0012468260589665002\n",
      "Surface training t=951, loss=0.0011836211716892962\n",
      "Surface training t=952, loss=0.0011729779260253417\n",
      "Surface training t=953, loss=0.001108549302103048\n",
      "Surface training t=954, loss=0.0012429716467081751\n",
      "Surface training t=955, loss=0.0011698885945093922\n",
      "Surface training t=956, loss=0.0011475952932579296\n",
      "Surface training t=957, loss=0.0011990783357664183\n",
      "Surface training t=958, loss=0.0011144034043063342\n",
      "Surface training t=959, loss=0.001169846370509554\n",
      "Surface training t=960, loss=0.0010502204024023922\n",
      "Surface training t=961, loss=0.0012496634793505755\n",
      "Surface training t=962, loss=0.0011892854010767998\n",
      "Surface training t=963, loss=0.0011781628248712572\n",
      "Surface training t=964, loss=0.001156217075662613\n",
      "Surface training t=965, loss=0.0011454828746170812\n",
      "Surface training t=966, loss=0.0010530732008185231\n",
      "Surface training t=967, loss=0.0010239475847513251\n",
      "Surface training t=968, loss=0.0011315444729547495\n",
      "Surface training t=969, loss=0.001063864023093671\n",
      "Surface training t=970, loss=0.0010345397547649468\n",
      "Surface training t=971, loss=0.001032936395484236\n",
      "Surface training t=972, loss=0.0010414594595884385\n",
      "Surface training t=973, loss=0.0011271673033517306\n",
      "Surface training t=974, loss=0.0011420707893344823\n",
      "Surface training t=975, loss=0.0010439398365485733\n",
      "Surface training t=976, loss=0.0011004354386060745\n",
      "Surface training t=977, loss=0.001063064306281882\n",
      "Surface training t=978, loss=0.001092597953930079\n",
      "Surface training t=979, loss=0.0011297756612292605\n",
      "Surface training t=980, loss=0.0010714092884480247\n",
      "Surface training t=981, loss=0.001039976253569501\n",
      "Surface training t=982, loss=0.0010633902956008797\n",
      "Surface training t=983, loss=0.001108995233008453\n",
      "Surface training t=984, loss=0.0010898796736548517\n",
      "Surface training t=985, loss=0.0009745747469816764\n",
      "Surface training t=986, loss=0.0010282549803665123\n",
      "Surface training t=987, loss=0.0010686428748327376\n",
      "Surface training t=988, loss=0.0010258346436615444\n",
      "Surface training t=989, loss=0.0010911426551613783\n",
      "Surface training t=990, loss=0.001058804708751183\n",
      "Surface training t=991, loss=0.0010680678831712443\n",
      "Surface training t=992, loss=0.0009660656945895446\n",
      "Surface training t=993, loss=0.0011591243113470893\n",
      "Surface training t=994, loss=0.0010207872820113364\n",
      "Surface training t=995, loss=0.0010649214868437157\n",
      "Surface training t=996, loss=0.0010321282398806006\n",
      "Surface training t=997, loss=0.001053327341074239\n",
      "Surface training t=998, loss=0.0009957144168956235\n",
      "Surface training t=999, loss=0.000958472609115492\n",
      "Surface training t=1000, loss=0.0010406904054005163\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2000\n",
    "# num_epochs = 2000\n",
    "num_epochs = 1000\n",
    "min_loss, t = np.inf, 0\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(train_features.size()[0])\n",
    "    loss_list = []\n",
    "    for i in range(0, len(train_target), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        # print(indices)\n",
    "        batch_x, target_y = train_features[indices], train_target[indices]\n",
    "        target_y = target_y.to(torch.float64)\n",
    "        optimizer.zero_grad()\n",
    "        output = b_model(batch_x)\n",
    "        # fpr, tpr, thresholds = roc_curve(target_y, output.detach().numpy())\n",
    "        # tresh = np.mean(thresholds)\n",
    "        # output = torch.round(output)\n",
    "        # output[output < tresh] = 0\n",
    "        # output[output != 0] = 1\n",
    "        # target_y = np.array(target_y, dtype=int)\n",
    "\n",
    "\n",
    "        # loss = -1 * torch.mean(torch.tensor(target_y) * torch.log(output) + torch.tensor((1 - target_y)) * torch.log(1 - output))\n",
    "        # loss = torch.mean(torch.abs(torch.tensor(target_y) - output))\n",
    "        loss = criterion(output, target_y.reshape_as(output))\n",
    "        \n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(loss.item())\n",
    "        loss_list.append(loss.item())\n",
    "    # print(loss_list)\n",
    "    loss_mean = np.mean(loss_list)\n",
    "\n",
    "    t += 1\n",
    "    print('Surface training t={}, loss={}'.format(t, loss_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr = b_model(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00],\n",
       "        [8.0870e-04],\n",
       "        [4.4106e-07],\n",
       "        [1.0000e+00],\n",
       "        [8.4816e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9983e-01],\n",
       "        [9.9919e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9978e-01],\n",
       "        [5.1414e-06],\n",
       "        [9.9998e-01],\n",
       "        [2.0010e-07],\n",
       "        [1.0000e+00],\n",
       "        [1.4939e-07],\n",
       "        [1.0000e+00],\n",
       "        [4.7520e-08],\n",
       "        [1.1601e-01],\n",
       "        [9.9999e-01],\n",
       "        [1.0836e-02],\n",
       "        [7.0599e-05],\n",
       "        [9.9004e-05],\n",
       "        [1.0000e+00],\n",
       "        [6.7268e-06],\n",
       "        [9.5173e-05],\n",
       "        [4.2931e-03],\n",
       "        [1.0781e-07],\n",
       "        [9.9999e-01],\n",
       "        [3.2123e-01],\n",
       "        [1.9762e-02],\n",
       "        [1.0000e+00],\n",
       "        [1.1335e-04],\n",
       "        [4.7102e-04],\n",
       "        [1.0000e+00],\n",
       "        [2.3816e-03],\n",
       "        [1.0000e+00],\n",
       "        [7.7497e-02],\n",
       "        [3.8397e-03],\n",
       "        [1.5473e-03],\n",
       "        [1.0000e+00],\n",
       "        [7.7944e-10],\n",
       "        [1.0000e+00],\n",
       "        [9.9958e-01],\n",
       "        [8.0458e-09],\n",
       "        [8.9560e-04],\n",
       "        [3.7712e-03],\n",
       "        [4.9586e-06],\n",
       "        [9.5928e-01],\n",
       "        [1.2630e-01],\n",
       "        [9.9993e-01],\n",
       "        [4.4257e-04],\n",
       "        [1.6126e-03],\n",
       "        [1.5697e-05],\n",
       "        [1.0198e-01],\n",
       "        [9.7129e-05],\n",
       "        [1.7122e-05],\n",
       "        [9.9992e-01],\n",
       "        [3.7291e-06],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9889e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.8956e-01],\n",
       "        [9.8495e-01],\n",
       "        [1.2689e-02],\n",
       "        [2.6893e-05],\n",
       "        [1.0000e+00],\n",
       "        [9.9997e-01],\n",
       "        [9.4697e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [1.1272e-07],\n",
       "        [9.4386e-06],\n",
       "        [5.0972e-05],\n",
       "        [3.3501e-04],\n",
       "        [1.0000e+00],\n",
       "        [2.9021e-05],\n",
       "        [1.0000e+00],\n",
       "        [9.9941e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [3.4321e-05],\n",
       "        [1.4334e-06],\n",
       "        [1.0356e-05],\n",
       "        [5.0722e-04],\n",
       "        [1.6518e-05],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9970e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.2193e-04],\n",
       "        [1.0000e+00],\n",
       "        [6.6854e-05],\n",
       "        [5.7004e-04],\n",
       "        [7.3087e-01],\n",
       "        [9.2815e-01],\n",
       "        [6.9640e-01],\n",
       "        [1.9407e-06],\n",
       "        [2.1975e-02],\n",
       "        [1.2458e-01],\n",
       "        [9.9973e-01],\n",
       "        [9.9998e-01],\n",
       "        [1.0000e+00],\n",
       "        [5.9754e-06],\n",
       "        [1.7941e-03],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [9.9840e-01],\n",
       "        [5.2069e-01],\n",
       "        [9.9618e-01],\n",
       "        [5.1738e-05],\n",
       "        [3.7236e-05],\n",
       "        [9.9951e-01],\n",
       "        [1.0000e+00],\n",
       "        [6.2256e-04],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [5.3500e-03],\n",
       "        [4.7732e-04],\n",
       "        [1.0000e+00],\n",
       "        [1.2554e-02],\n",
       "        [1.6591e-05],\n",
       "        [2.5411e-08],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [4.8293e-06],\n",
       "        [1.0000e+00],\n",
       "        [9.9293e-01],\n",
       "        [9.7637e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9997e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.7869e-06],\n",
       "        [2.0787e-07],\n",
       "        [1.0000e+00],\n",
       "        [9.9996e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [4.2287e-02],\n",
       "        [6.6369e-05],\n",
       "        [4.6356e-07],\n",
       "        [5.3972e-07],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9998e-01],\n",
       "        [5.6858e-06],\n",
       "        [5.4439e-02],\n",
       "        [4.0310e-01],\n",
       "        [2.1412e-03],\n",
       "        [9.9995e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9984e-01],\n",
       "        [1.0028e-05],\n",
       "        [1.4524e-02],\n",
       "        [1.9094e-04],\n",
       "        [1.0000e+00],\n",
       "        [1.4562e-06],\n",
       "        [1.6524e-07],\n",
       "        [9.9997e-01],\n",
       "        [9.9955e-01],\n",
       "        [4.8026e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.2038e-08],\n",
       "        [1.0000e+00],\n",
       "        [3.9095e-06],\n",
       "        [8.6960e-04],\n",
       "        [1.6070e-07],\n",
       "        [6.5871e-10],\n",
       "        [1.0000e+00],\n",
       "        [5.2395e-05],\n",
       "        [1.0488e-05],\n",
       "        [3.2313e-02],\n",
       "        [9.8663e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.6918e-05],\n",
       "        [2.6305e-02],\n",
       "        [1.0000e+00],\n",
       "        [6.4395e-04],\n",
       "        [1.1946e-06],\n",
       "        [1.7730e-01],\n",
       "        [9.9999e-01],\n",
       "        [4.5341e-06],\n",
       "        [3.6388e-03],\n",
       "        [1.0000e+00],\n",
       "        [1.2641e-04],\n",
       "        [7.4916e-07],\n",
       "        [9.1327e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.6774e-01],\n",
       "        [8.9385e-03],\n",
       "        [2.5861e-05],\n",
       "        [1.0000e+00],\n",
       "        [9.9973e-01],\n",
       "        [9.2436e-01],\n",
       "        [8.9925e-03],\n",
       "        [4.1443e-06],\n",
       "        [1.0282e-04],\n",
       "        [9.9998e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.5305e-01],\n",
       "        [9.9939e-01],\n",
       "        [9.9999e-01],\n",
       "        [9.6448e-01],\n",
       "        [1.0647e-04],\n",
       "        [3.3356e-06],\n",
       "        [5.1121e-01],\n",
       "        [2.1808e-06],\n",
       "        [8.5891e-03],\n",
       "        [9.9225e-01],\n",
       "        [1.7864e-01],\n",
       "        [9.9583e-01],\n",
       "        [9.8241e-01],\n",
       "        [9.9212e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9994e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.2311e-06],\n",
       "        [1.0000e+00],\n",
       "        [9.9882e-01],\n",
       "        [1.0000e+00],\n",
       "        [6.7602e-04],\n",
       "        [9.8938e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [2.0899e-02],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [2.0184e-04],\n",
       "        [1.0060e-02],\n",
       "        [9.9997e-01],\n",
       "        [6.1805e-10],\n",
       "        [9.8183e-01],\n",
       "        [1.0000e+00],\n",
       "        [7.4781e-07],\n",
       "        [9.6613e-06],\n",
       "        [9.9898e-01],\n",
       "        [5.4753e-09],\n",
       "        [9.9989e-01],\n",
       "        [1.1447e-03],\n",
       "        [1.0842e-02],\n",
       "        [9.9999e-01],\n",
       "        [9.9998e-01],\n",
       "        [1.1043e-07],\n",
       "        [2.1075e-02],\n",
       "        [5.3951e-05],\n",
       "        [3.2818e-02],\n",
       "        [8.6544e-05],\n",
       "        [1.0518e-01],\n",
       "        [9.0387e-01],\n",
       "        [9.9999e-01],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [6.7646e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.5402e-01],\n",
       "        [2.4804e-06],\n",
       "        [1.8841e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [8.2528e-01],\n",
       "        [6.2397e-06],\n",
       "        [9.9993e-01],\n",
       "        [1.0000e+00],\n",
       "        [3.5807e-07],\n",
       "        [2.8473e-03],\n",
       "        [8.9337e-08],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [7.5793e-06],\n",
       "        [9.8737e-01],\n",
       "        [2.3456e-06],\n",
       "        [1.0000e+00],\n",
       "        [9.9941e-01],\n",
       "        [4.1857e-03],\n",
       "        [2.0148e-06],\n",
       "        [1.5558e-05],\n",
       "        [4.2757e-02],\n",
       "        [1.6568e-02],\n",
       "        [9.9726e-01],\n",
       "        [1.0000e+00],\n",
       "        [8.1567e-08],\n",
       "        [2.9560e-01],\n",
       "        [6.6386e-07],\n",
       "        [6.9960e-01],\n",
       "        [1.8078e-05],\n",
       "        [6.6728e-01],\n",
       "        [2.1241e-04],\n",
       "        [9.9999e-01],\n",
       "        [8.4971e-01],\n",
       "        [9.8533e-01],\n",
       "        [3.2101e-06],\n",
       "        [1.0000e+00],\n",
       "        [9.6549e-01],\n",
       "        [1.3519e-04],\n",
       "        [1.0000e+00],\n",
       "        [3.1048e-02],\n",
       "        [1.3528e-05],\n",
       "        [9.9926e-01],\n",
       "        [9.9998e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.2249e-05],\n",
       "        [9.9995e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.1289e-04],\n",
       "        [6.5226e-08],\n",
       "        [1.0000e+00],\n",
       "        [9.9993e-01],\n",
       "        [3.2228e-06],\n",
       "        [5.1862e-01],\n",
       "        [9.9994e-01],\n",
       "        [3.5785e-03],\n",
       "        [9.9993e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [6.0977e-04],\n",
       "        [3.7081e-06],\n",
       "        [9.9823e-01],\n",
       "        [2.7293e-01],\n",
       "        [1.3879e-05],\n",
       "        [9.8299e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9739e-01],\n",
       "        [1.0237e-04],\n",
       "        [1.5735e-03],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.8317e-01],\n",
       "        [2.8881e-03],\n",
       "        [1.2182e-05],\n",
       "        [3.0857e-04],\n",
       "        [3.3393e-01],\n",
       "        [1.0000e+00],\n",
       "        [2.5679e-03],\n",
       "        [1.8428e-03],\n",
       "        [2.6947e-04],\n",
       "        [5.9985e-05],\n",
       "        [1.0534e-08],\n",
       "        [9.9996e-01],\n",
       "        [9.9890e-01],\n",
       "        [9.9990e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [3.6914e-05],\n",
       "        [3.3517e-05],\n",
       "        [9.9984e-01],\n",
       "        [7.9821e-05],\n",
       "        [1.0000e+00],\n",
       "        [5.4549e-04],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9973e-01],\n",
       "        [9.8214e-06],\n",
       "        [9.9997e-01],\n",
       "        [3.4216e-02],\n",
       "        [5.2552e-07],\n",
       "        [1.0000e+00],\n",
       "        [4.5592e-09],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [4.9969e-01],\n",
       "        [1.1452e-01],\n",
       "        [1.4819e-02],\n",
       "        [9.9653e-01],\n",
       "        [2.0110e-05],\n",
       "        [9.9994e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [3.6007e-10],\n",
       "        [9.9999e-01],\n",
       "        [1.3751e-06],\n",
       "        [4.8179e-05],\n",
       "        [1.5658e-01],\n",
       "        [1.8215e-01],\n",
       "        [6.8605e-07],\n",
       "        [1.6331e-03],\n",
       "        [7.5364e-10],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [8.0802e-04],\n",
       "        [3.1071e-05],\n",
       "        [9.5743e-01],\n",
       "        [9.2784e-03],\n",
       "        [1.3372e-08],\n",
       "        [1.0000e+00],\n",
       "        [7.9165e-06],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.8179e-09],\n",
       "        [9.8501e-01],\n",
       "        [9.9985e-01],\n",
       "        [9.9997e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.6801e-02],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.3316e-06],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [3.6564e-03],\n",
       "        [9.9999e-01],\n",
       "        [9.9983e-01],\n",
       "        [1.6909e-05],\n",
       "        [9.9995e-01],\n",
       "        [9.9975e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.5605e-06],\n",
       "        [9.7762e-01],\n",
       "        [9.1736e-05],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.5556e-02],\n",
       "        [4.9487e-05],\n",
       "        [6.8467e-03],\n",
       "        [4.3832e-05],\n",
       "        [9.3117e-04],\n",
       "        [4.6444e-02],\n",
       "        [5.6701e-04],\n",
       "        [1.0000e+00],\n",
       "        [1.8371e-05],\n",
       "        [8.8555e-01],\n",
       "        [1.0000e+00],\n",
       "        [2.5731e-09],\n",
       "        [1.0026e-05],\n",
       "        [9.7503e-01],\n",
       "        [1.5590e-03],\n",
       "        [1.0000e+00],\n",
       "        [9.9972e-01],\n",
       "        [2.8617e-02],\n",
       "        [1.0962e-07],\n",
       "        [9.9994e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [4.4011e-06],\n",
       "        [3.7617e-10],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [7.8428e-03],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [4.5235e-05],\n",
       "        [9.7692e-01],\n",
       "        [1.9043e-05],\n",
       "        [4.6641e-05],\n",
       "        [7.1997e-01],\n",
       "        [6.4258e-10],\n",
       "        [1.2573e-10],\n",
       "        [4.9054e-05],\n",
       "        [1.0000e+00],\n",
       "        [2.7678e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [6.7158e-05],\n",
       "        [4.3157e-08],\n",
       "        [1.0000e+00],\n",
       "        [9.0852e-02],\n",
       "        [2.4012e-05],\n",
       "        [8.0041e-04],\n",
       "        [6.9828e-01],\n",
       "        [1.4226e-05],\n",
       "        [3.4973e-03],\n",
       "        [9.9845e-01],\n",
       "        [1.8425e-02],\n",
       "        [9.9998e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [9.8523e-01],\n",
       "        [7.2460e-08],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.5623e-01],\n",
       "        [9.9999e-01],\n",
       "        [1.2145e-05],\n",
       "        [7.2487e-10],\n",
       "        [1.0000e+00],\n",
       "        [5.0786e-02],\n",
       "        [9.9953e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [5.5262e-07],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [2.1561e-06],\n",
       "        [1.4839e-05],\n",
       "        [8.2110e-05],\n",
       "        [5.4425e-04],\n",
       "        [1.0000e+00],\n",
       "        [2.6124e-07],\n",
       "        [1.0000e+00],\n",
       "        [9.5523e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [6.3124e-08],\n",
       "        [9.9867e-01],\n",
       "        [9.9994e-01],\n",
       "        [2.3057e-08],\n",
       "        [6.4129e-01],\n",
       "        [9.9998e-01],\n",
       "        [2.0523e-07],\n",
       "        [1.0000e+00],\n",
       "        [2.9689e-06],\n",
       "        [1.0000e+00],\n",
       "        [5.6968e-07],\n",
       "        [3.4177e-05],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [7.1728e-02],\n",
       "        [1.9237e-05],\n",
       "        [4.3018e-03],\n",
       "        [2.0353e-02],\n",
       "        [1.0000e+00],\n",
       "        [9.7845e-01],\n",
       "        [4.0797e-02],\n",
       "        [1.0000e+00],\n",
       "        [5.9938e-05],\n",
       "        [4.5098e-09],\n",
       "        [9.7253e-10],\n",
       "        [9.8698e-05],\n",
       "        [2.4587e-04],\n",
       "        [6.7579e-04],\n",
       "        [9.9976e-01],\n",
       "        [3.9507e-09],\n",
       "        [1.7683e-02],\n",
       "        [1.0000e+00],\n",
       "        [3.2666e-02],\n",
       "        [8.1977e-05],\n",
       "        [1.2066e-06],\n",
       "        [9.9998e-01],\n",
       "        [7.2216e-09],\n",
       "        [9.9998e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0374e-04],\n",
       "        [1.0000e+00],\n",
       "        [1.9595e-09],\n",
       "        [1.0000e+00],\n",
       "        [1.7717e-04],\n",
       "        [1.0000e+00],\n",
       "        [9.4878e-01],\n",
       "        [6.8762e-06],\n",
       "        [7.4507e-07],\n",
       "        [9.9967e-01],\n",
       "        [9.8671e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.7191e-05],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [2.8650e-01],\n",
       "        [1.9484e-07],\n",
       "        [1.0000e+00],\n",
       "        [9.9936e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.3204e-01],\n",
       "        [1.0073e-06],\n",
       "        [1.0000e+00],\n",
       "        [1.9104e-06],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [2.6168e-06],\n",
       "        [6.6493e-04],\n",
       "        [1.3910e-03],\n",
       "        [9.9998e-01],\n",
       "        [1.0000e+00],\n",
       "        [2.0337e-03],\n",
       "        [9.7714e-01],\n",
       "        [3.8713e-04],\n",
       "        [3.7380e-02],\n",
       "        [8.6859e-01],\n",
       "        [9.9820e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9276e-01],\n",
       "        [1.0000e+00],\n",
       "        [2.7966e-05],\n",
       "        [1.2498e-09],\n",
       "        [8.4827e-09],\n",
       "        [1.2543e-04],\n",
       "        [6.8684e-08],\n",
       "        [9.8755e-01],\n",
       "        [9.9965e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9824e-01],\n",
       "        [4.9991e-06],\n",
       "        [2.9268e-04],\n",
       "        [3.2220e-09],\n",
       "        [6.0801e-08],\n",
       "        [2.2484e-06],\n",
       "        [1.0000e+00],\n",
       "        [1.1033e-04],\n",
       "        [9.9623e-05],\n",
       "        [2.5378e-10],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9995e-01],\n",
       "        [1.0132e-05],\n",
       "        [8.7286e-09],\n",
       "        [2.8880e-06],\n",
       "        [6.8356e-01],\n",
       "        [9.4853e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9763e-06],\n",
       "        [1.0000e+00],\n",
       "        [2.7924e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.1938e-03],\n",
       "        [9.9958e-01],\n",
       "        [9.1762e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.8301e-07],\n",
       "        [7.2101e-06],\n",
       "        [6.7016e-04],\n",
       "        [9.9183e-01],\n",
       "        [9.9999e-01],\n",
       "        [9.9958e-01],\n",
       "        [8.9675e-01],\n",
       "        [9.9999e-01],\n",
       "        [9.9473e-01],\n",
       "        [6.8740e-06],\n",
       "        [3.2061e-06],\n",
       "        [9.9340e-01],\n",
       "        [1.1467e-10],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [5.4800e-01],\n",
       "        [9.2113e-09],\n",
       "        [2.8443e-03],\n",
       "        [1.4410e-04],\n",
       "        [4.0321e-05],\n",
       "        [1.0000e+00],\n",
       "        [6.7997e-05],\n",
       "        [1.6431e-04],\n",
       "        [6.4107e-07],\n",
       "        [9.9990e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.5512e-05],\n",
       "        [9.9999e-01],\n",
       "        [3.1771e-08],\n",
       "        [9.7470e-01],\n",
       "        [6.4567e-01],\n",
       "        [3.8644e-02],\n",
       "        [1.0000e+00],\n",
       "        [9.7287e-01],\n",
       "        [1.6693e-11],\n",
       "        [9.5298e-01],\n",
       "        [9.9199e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9873e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.8333e-05],\n",
       "        [4.3908e-09],\n",
       "        [1.0000e+00],\n",
       "        [2.7576e-02],\n",
       "        [1.2237e-01],\n",
       "        [6.1273e-08],\n",
       "        [1.1276e-08],\n",
       "        [7.4821e-07],\n",
       "        [1.0000e+00],\n",
       "        [1.0502e-08],\n",
       "        [5.1930e-06],\n",
       "        [4.1601e-08],\n",
       "        [1.0000e+00],\n",
       "        [4.2047e-05],\n",
       "        [9.9998e-01],\n",
       "        [5.2262e-03],\n",
       "        [1.2581e-05],\n",
       "        [1.0000e+00],\n",
       "        [9.9952e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [3.3073e-05],\n",
       "        [1.8750e-05],\n",
       "        [2.3211e-08],\n",
       "        [9.4023e-02],\n",
       "        [2.0500e-04],\n",
       "        [1.0000e+00],\n",
       "        [2.1807e-02],\n",
       "        [9.9990e-01],\n",
       "        [5.3476e-06],\n",
       "        [6.4059e-02],\n",
       "        [9.8980e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9988e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [2.6544e-06],\n",
       "        [8.0251e-05],\n",
       "        [2.3281e-09],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9865e-01],\n",
       "        [5.2588e-06],\n",
       "        [1.0804e-02],\n",
       "        [7.8096e-07],\n",
       "        [8.5385e-01],\n",
       "        [3.3686e-08],\n",
       "        [1.7778e-04],\n",
       "        [2.7298e-04],\n",
       "        [9.9937e-01],\n",
       "        [9.9999e-01],\n",
       "        [9.9955e-01],\n",
       "        [2.0952e-08],\n",
       "        [1.3280e-06],\n",
       "        [5.3152e-09],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0227e-04],\n",
       "        [5.5262e-01],\n",
       "        [1.0904e-01],\n",
       "        [3.2336e-06],\n",
       "        [9.7952e-01],\n",
       "        [9.9995e-01],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [2.7735e-02],\n",
       "        [6.9821e-06],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [7.6914e-04],\n",
       "        [6.2687e-06],\n",
       "        [1.4599e-03],\n",
       "        [5.3306e-01],\n",
       "        [1.9734e-04],\n",
       "        [1.8967e-06],\n",
       "        [9.3472e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9995e-01],\n",
       "        [2.9824e-05],\n",
       "        [2.7066e-02],\n",
       "        [1.3788e-01],\n",
       "        [9.9587e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9985e-01],\n",
       "        [3.5338e-03],\n",
       "        [1.0000e+00],\n",
       "        [9.9967e-01],\n",
       "        [2.0354e-06],\n",
       "        [1.2754e-05],\n",
       "        [1.0000e+00],\n",
       "        [4.8574e-02],\n",
       "        [2.2449e-06],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [3.3256e-07],\n",
       "        [1.0000e+00],\n",
       "        [9.9684e-01],\n",
       "        [1.1677e-06],\n",
       "        [9.1088e-01],\n",
       "        [9.7286e-01],\n",
       "        [3.9233e-03],\n",
       "        [1.0000e+00],\n",
       "        [3.7034e-04],\n",
       "        [7.9153e-02],\n",
       "        [8.8996e-05],\n",
       "        [1.1856e-05],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [7.5040e-04],\n",
       "        [9.9999e-01],\n",
       "        [1.0047e-05]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9587, dtype=torch.float64, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_y = torch.tensor(test_target.to_numpy()).to(torch.float64)\n",
    "criterion(ttr, target_y.reshape_as(ttr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.00 GB: 100%|██████████| 3/3 [00:02<00:00,  1.20it/s]\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/hiv.zip\n",
      "Loading necessary files...\n",
      "This might take a while.\n",
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41127/41127 [00:00<00:00, 112284.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41127/41127 [00:01<00:00, 35330.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "c:\\Python39\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Download and process data at './dataset/ogbg_molhiv/'\n",
    "dataset = PygGraphPropPredDataset(name = \"ogbg-molhiv\", root = 'dataset/')\n",
    "\n",
    " \n",
    "split_idx = dataset.get_idx_split() \n",
    "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.BatchSampler at 0x24e1608c6a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    3,     4,     5,  ..., 41124, 41125, 41126])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_points, sr_color = datasets.make_swiss_roll(n_samples=1500, random_state=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
